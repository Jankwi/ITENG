epoch 0  training_set_loss 2385.79534999237057490973  testing_set_loss 3056.02694595158709489624
epoch 1  training_set_loss 2376.02605078626083923155  testing_set_loss 3056.81494529847577723558
epoch 2  training_set_loss 2369.40607349289621197386  testing_set_loss 3056.96618304110461394885
epoch 3  training_set_loss 2365.11769503979212458944  testing_set_loss 3057.00678404638574647834
epoch 4  training_set_loss 2361.81307536462372809183  testing_set_loss 3057.19080714331266790396
epoch 5  training_set_loss 2359.14972828056352227577  testing_set_loss 3057.05943536089262124733
epoch 6  training_set_loss 2356.94981631894779638969  testing_set_loss 3056.78966317017193432548
epoch 7  training_set_loss 2355.07899009790935451747  testing_set_loss 3055.95549188367704118718
epoch 8  training_set_loss 2354.21068063815118875937  testing_set_loss 3055.62831970645765977679
epoch 9  training_set_loss 2353.34965611952247854788  testing_set_loss 3055.30976546651663738885
epoch 10  training_set_loss 2352.40794027377160091419  testing_set_loss 3054.16241794531651976286
epoch 11  training_set_loss 2351.59836300252118235221  testing_set_loss 3053.88365363762068227516
epoch 12  training_set_loss 2350.86647548815426489455  testing_set_loss 3053.09941197985972394235
epoch 13  training_set_loss 2350.41633126898977934616  testing_set_loss 3052.37282849326720679528
epoch 14  training_set_loss 2349.92769938390347306267  testing_set_loss 3051.83038514317740919068
epoch 15  training_set_loss 2349.40600287686083902372  testing_set_loss 3051.41688007453876707586
epoch 16  training_set_loss 2348.82164902714475829271  testing_set_loss 3050.88414482763755586348
epoch 17  training_set_loss 2348.20623403921081262524  testing_set_loss 3050.35552983802381277201
epoch 18  training_set_loss 2347.79439312890463043004  testing_set_loss 3050.25500018275079128216
epoch 19  training_set_loss 2347.35205368881952381344  testing_set_loss 3049.58026387243944554939
epoch 20  training_set_loss 2346.82159210052896014531  testing_set_loss 3049.02452655224442423787
epoch 21  training_set_loss 2346.18833940798049297882  testing_set_loss 3048.38596394647811393952
epoch 22  training_set_loss 2345.40691584634032551548  testing_set_loss 3047.22572286385957340826
epoch 23  training_set_loss 2344.88795420820088111213  testing_set_loss 3046.98137161252498117392
epoch 24  training_set_loss 2344.28474309846615142305  testing_set_loss 3046.56992733019342267653
epoch 25  training_set_loss 2343.61649749813022935996  testing_set_loss 3045.85403500924394393223
epoch 26  training_set_loss 2342.77235420549141053925  testing_set_loss 3044.88591752075080876239
epoch 27  training_set_loss 2342.26441786590748961316  testing_set_loss 3044.46764813586150921765
epoch 28  training_set_loss 2341.64263156700553736300  testing_set_loss 3043.96492309883433335926
epoch 29  training_set_loss 2340.83995520058624606463  testing_set_loss 3043.28981043657540794811
epoch 30  training_set_loss 2339.95550040722582707531  testing_set_loss 3042.95638335278454178479
epoch 31  training_set_loss 2339.03600179330442188075  testing_set_loss 3042.21364618559118753183
epoch 32  training_set_loss 2338.57930334963248242275  testing_set_loss 3041.93763503606351150665
epoch 33  training_set_loss 2338.15276017926316853845  testing_set_loss 3041.90755163025960428058
epoch 34  training_set_loss 2337.74428908475465505035  testing_set_loss 3041.40651300469289708417
epoch 35  training_set_loss 2337.36231248871035859338  testing_set_loss 3041.18803863832545175683
epoch 36  training_set_loss 2336.96737901298592987587  testing_set_loss 3041.16485107351672922960
epoch 37  training_set_loss 2336.74058886585771688260  testing_set_loss 3041.14814736431071651168
epoch 38  training_set_loss 2336.45390669209700718056  testing_set_loss 3041.19838596472391145653
epoch 39  training_set_loss 2336.12277521118812728673  testing_set_loss 3041.03536194409070958500
epoch 40  training_set_loss 2335.73255934577446168987  testing_set_loss 3040.97436275162499441649
epoch 41  training_set_loss 2335.27951794510909167002  testing_set_loss 3040.62976171599120789324
epoch 42  training_set_loss 2334.99441471150339566520  testing_set_loss 3040.44366119605274434434
epoch 43  training_set_loss 2334.60195687018995158724  testing_set_loss 3040.03833079831656505121
epoch 44  training_set_loss 2334.12586942270127110532  testing_set_loss 3040.05089121053060807753
epoch 45  training_set_loss 2333.46937088144886729424  testing_set_loss 3039.90721445725375815528
epoch 46  training_set_loss 2332.66254454852378330543  testing_set_loss 3039.79932026503229280934
epoch 47  training_set_loss 2332.11666097195893598837  testing_set_loss 3039.92986885191066903644
epoch 48  training_set_loss 2331.46075194344393821666  testing_set_loss 3039.60800979099803953432
epoch 49  training_set_loss 2330.71061055710651999107  testing_set_loss 3039.27983852645047591068
epoch 50  training_set_loss 2329.88076855457074998412  testing_set_loss 3038.63268922808083516429
epoch 51  training_set_loss 2329.43255530485248527839  testing_set_loss 3038.22724370127434667666
epoch 52  training_set_loss 2328.90925160132337623509  testing_set_loss 3037.57424154125328641385
epoch 53  training_set_loss 2328.33652438098897619057  testing_set_loss 3036.91592382417138651363
epoch 54  training_set_loss 2327.74838445893738025916  testing_set_loss 3036.33867771188261031057
epoch 55  training_set_loss 2327.10749522683818213409  testing_set_loss 3035.49917534822270681616
epoch 56  training_set_loss 2326.69789078005305782426  testing_set_loss 3035.01618505018223004299
epoch 57  training_set_loss 2326.18262337479563939269  testing_set_loss 3034.41059344152472476708
epoch 58  training_set_loss 2325.55831309293353115208  testing_set_loss 3033.56823972781558040879
epoch 59  training_set_loss 2324.77691522143959446112  testing_set_loss 3032.38549877468449267326
epoch 60  training_set_loss 2323.73246842940943679423  testing_set_loss 3031.08704126059546979377
epoch 61  training_set_loss 2322.98174975155143329175  testing_set_loss 3030.11823140614660587744
epoch 62  training_set_loss 2321.90704960665425460320  testing_set_loss 3028.82878984790022514062
epoch 63  training_set_loss 2320.50050062586660715169  testing_set_loss 3027.12315684453915309859
epoch 64  training_set_loss 2318.61389791692727158079  testing_set_loss 3025.48023220890172524378
epoch 65  training_set_loss 2316.54204680996872411924  testing_set_loss 3023.48534178828549556783
epoch 66  training_set_loss 2315.55637796749124390772  testing_set_loss 3022.65155376363190953271
epoch 67  training_set_loss 2314.52192970843680086546  testing_set_loss 3021.84049222668136280845
epoch 68  training_set_loss 2313.24464644141880853567  testing_set_loss 3021.30572036650210066000
epoch 69  training_set_loss 2311.64107457486898056231  testing_set_loss 3020.24156274630831831018
epoch 70  training_set_loss 2310.00929962696500297170  testing_set_loss 3019.69510570386091785622
epoch 71  training_set_loss 2309.31549405299165300676  testing_set_loss 3019.53108313206212187652
epoch 72  training_set_loss 2308.47812094919845549157  testing_set_loss 3019.26338626550796107040
epoch 73  training_set_loss 2307.60429399013946749619  testing_set_loss 3018.56179765410070103826
epoch 74  training_set_loss 2306.67402963963741058251  testing_set_loss 3018.05308445148148166481
epoch 75  training_set_loss 2306.18645386304706335068  testing_set_loss 3018.03712686755488903145
epoch 76  training_set_loss 2305.67595150028500938788  testing_set_loss 3017.83239980977577943122
epoch 77  training_set_loss 2305.15692318032915864023  testing_set_loss 3017.62866364917999817408
epoch 78  training_set_loss 2304.66618941944079779205  testing_set_loss 3017.28941869860773294931
epoch 79  training_set_loss 2304.18092311686723405728  testing_set_loss 3017.15462164877226314275
epoch 80  training_set_loss 2303.90821401410084945383  testing_set_loss 3017.04043687836110620992
epoch 81  training_set_loss 2303.64772490803352411604  testing_set_loss 3016.95098986329821855179
epoch 82  training_set_loss 2303.36303053520441608271  testing_set_loss 3016.67970486667309160111
epoch 83  training_set_loss 2303.05288925597687921254  testing_set_loss 3016.65466971947853380698
epoch 84  training_set_loss 2302.72410838869927829364  testing_set_loss 3016.49364417009519456769
epoch 85  training_set_loss 2302.52662630102031471324  testing_set_loss 3016.23312357392842386616
epoch 86  training_set_loss 2302.31312312145291798515  testing_set_loss 3016.20167318991207139334
epoch 87  training_set_loss 2302.07476350961769639980  testing_set_loss 3016.21738181628052188898
epoch 88  training_set_loss 2301.81343879129826746066  testing_set_loss 3015.85026754269756565918
epoch 89  training_set_loss 2301.51140202249916910660  testing_set_loss 3015.66164904538618429797
epoch 90  training_set_loss 2301.32901936900634609628  testing_set_loss 3015.63096064078763447469
epoch 91  training_set_loss 2301.11819248011443050927  testing_set_loss 3015.63794505056739581050
epoch 92  training_set_loss 2300.88588648485483645345  testing_set_loss 3015.75471716112406284083
epoch 93  training_set_loss 2300.61980694938438318786  testing_set_loss 3015.91613042538074296317
epoch 94  training_set_loss 2300.31532072278923806152  testing_set_loss 3015.83591228071145451395
epoch 95  training_set_loss 2300.12782409609144451679  testing_set_loss 3015.70998582157017153804
epoch 96  training_set_loss 2299.90855131537819033838  testing_set_loss 3015.87757895023059973028
epoch 97  training_set_loss 2299.63605647764734385419  testing_set_loss 3015.95723720241858245572
epoch 98  training_set_loss 2299.29563864213605484110  testing_set_loss 3015.66365659900111495517
epoch 99  training_set_loss 2299.10478205598838030710  testing_set_loss 3015.49984661190728729707
epoch 100  training_set_loss 2298.84287097724154591560  testing_set_loss 3015.19126149461544628139
epoch 101  training_set_loss 2298.54097327430281438865  testing_set_loss 3015.29378899592438756372
epoch 102  training_set_loss 2298.13892018030765029835  testing_set_loss 3015.37870321644368232228
epoch 103  training_set_loss 2297.58505565735777054215  testing_set_loss 3015.18152783244477177504
epoch 104  training_set_loss 2297.21754961960095897666  testing_set_loss 3015.07439431772309035296
epoch 105  training_set_loss 2296.73038589598263570224  testing_set_loss 3014.97657871333831280936
epoch 106  training_set_loss 2296.09007150464549340541  testing_set_loss 3014.76705221695192449261
epoch 107  training_set_loss 2295.21137821406273360481  testing_set_loss 3014.70287316701251256745
epoch 108  training_set_loss 2294.05183623723178243381  testing_set_loss 3014.57189062511224619811
epoch 109  training_set_loss 2293.35227169963991400436  testing_set_loss 3014.29375081020407378674
epoch 110  training_set_loss 2292.52834191602596547455  testing_set_loss 3013.95062931391157690086
epoch 111  training_set_loss 2291.54895621490277335397  testing_set_loss 3013.67073014525522012264
epoch 112  training_set_loss 2290.45727133051696000621  testing_set_loss 3013.41773562642083561514
epoch 113  training_set_loss 2289.26672394282923050923  testing_set_loss 3013.26814432214632688556
epoch 114  training_set_loss 2288.65943999389855889603  testing_set_loss 3013.43226504594395009917
epoch 115  training_set_loss 2288.00444572454716762877  testing_set_loss 3013.75632607346051372588
epoch 116  training_set_loss 2287.25921748435484914808  testing_set_loss 3013.86498595046168702538
epoch 117  training_set_loss 2286.50516107039175039972  testing_set_loss 3014.27229824207233832567
epoch 118  training_set_loss 2285.63401206978687696392  testing_set_loss 3014.22452159382055469905
epoch 119  training_set_loss 2285.16294858145920443349  testing_set_loss 3014.66196651675727480324
epoch 120  training_set_loss 2284.57208228598710775259  testing_set_loss 3015.16920941790795041015
epoch 121  training_set_loss 2283.80479180159318275400  testing_set_loss 3015.72134883431772323092
epoch 122  training_set_loss 2282.78322622350469828234  testing_set_loss 3016.22679970252374914708
epoch 123  training_set_loss 2282.15608900775805523153  testing_set_loss 3016.44714237546531876433
epoch 124  training_set_loss 2281.45188067478238735930  testing_set_loss 3016.91637080883174348855
epoch 125  training_set_loss 2280.69717371069600631017  testing_set_loss 3017.23825632748685166007
epoch 126  training_set_loss 2279.85246126520451070974  testing_set_loss 3017.17093071150156902149
epoch 127  training_set_loss 2278.85049652227053229581  testing_set_loss 3017.12461987492633852526
epoch 128  training_set_loss 2278.20742402159794437466  testing_set_loss 3016.97331092444392197649
epoch 129  training_set_loss 2277.36288179309394763550  testing_set_loss 3017.09294244402235563030
epoch 130  training_set_loss 2276.33858288481951603899  testing_set_loss 3016.86312181965149648022
epoch 131  training_set_loss 2275.06785211712394811912  testing_set_loss 3016.99850205902248490020
epoch 132  training_set_loss 2273.69907524591917535872  testing_set_loss 3016.73988512702953812550
epoch 133  training_set_loss 2273.02235726709295704495  testing_set_loss 3016.47922032684846271877
epoch 134  training_set_loss 2272.37135572649958703551  testing_set_loss 3016.18588196166183479363
epoch 135  training_set_loss 2271.62609350747152348049  testing_set_loss 3015.99683331107371486723
epoch 136  training_set_loss 2270.89750014714218195877  testing_set_loss 3015.55214352217717532767
epoch 137  training_set_loss 2270.11689835875540666166  testing_set_loss 3015.64243223673520333250
epoch 138  training_set_loss 2269.72979808546733693220  testing_set_loss 3015.37827809353211705456
epoch 139  training_set_loss 2269.29542874218032011413  testing_set_loss 3015.08574726321012349217
epoch 140  training_set_loss 2268.85119024568984968937  testing_set_loss 3015.02260779252583233756
epoch 141  training_set_loss 2268.33954720504561919370  testing_set_loss 3014.77415366660125073395
epoch 142  training_set_loss 2267.82143783689525662339  testing_set_loss 3014.65618470809022255708
epoch 143  training_set_loss 2267.55669529712213261519  testing_set_loss 3014.77233420050015411107
epoch 144  training_set_loss 2267.27722249419957734062  testing_set_loss 3014.73081143951367266709
epoch 145  training_set_loss 2266.99723898009415279375  testing_set_loss 3014.62083781153387462837
epoch 146  training_set_loss 2266.69626142078095654142  testing_set_loss 3014.63013473171349687618
epoch 147  training_set_loss 2266.54492068563286011340  testing_set_loss 3014.50784241921837747213
epoch 148  training_set_loss 2266.36986322188340636785  testing_set_loss 3014.53003111995167273562
epoch 149  training_set_loss 2266.18449454780511587160  testing_set_loss 3014.47476449783835050766
epoch 150  training_set_loss 2265.97647162235443829559  testing_set_loss 3014.41419373380767865456
epoch 151  training_set_loss 2265.76130437769870695774  testing_set_loss 3014.43019689337052113842
epoch 152  training_set_loss 2265.64250264949350821553  testing_set_loss 3014.49259207817431160947
epoch 153  training_set_loss 2265.50343703772796288831  testing_set_loss 3014.70012946876704518218
epoch 154  training_set_loss 2265.34891357456672267290  testing_set_loss 3014.71416379113725270145
epoch 155  training_set_loss 2265.17184206502224697033  testing_set_loss 3014.70923405230996650062
epoch 156  training_set_loss 2264.97064311942585845827  testing_set_loss 3014.65181760491668683244
epoch 157  training_set_loss 2264.86490772352044587024  testing_set_loss 3014.53968613135475607123
epoch 158  training_set_loss 2264.72771975086379825370  testing_set_loss 3014.37947375279964035144
epoch 159  training_set_loss 2264.56204302030664621270  testing_set_loss 3014.13759798475030038389
epoch 160  training_set_loss 2264.37607808957363886293  testing_set_loss 3013.90424689699011651101
epoch 161  training_set_loss 2264.13245883666695590364  testing_set_loss 3013.73841899507033303962
epoch 162  training_set_loss 2263.99834121684898491367  testing_set_loss 3013.73645131509738348541
epoch 163  training_set_loss 2263.79602860298746236367  testing_set_loss 3013.64317982367538206745
epoch 164  training_set_loss 2263.55508859005749400239  testing_set_loss 3013.61300572479149195715
epoch 165  training_set_loss 2263.23083162349121266743  testing_set_loss 3013.69265017644329418545
epoch 166  training_set_loss 2262.80131761674601875711  testing_set_loss 3013.72325083015675772913
epoch 167  training_set_loss 2262.51991561247405115864  testing_set_loss 3013.69903560688226207276
epoch 168  training_set_loss 2262.21602704014594564796  testing_set_loss 3013.75989137079113788786
epoch 169  training_set_loss 2261.86675705791685686563  testing_set_loss 3013.82614704065599653404
epoch 170  training_set_loss 2261.51033274017845542403  testing_set_loss 3013.89599781185097526759
epoch 171  training_set_loss 2261.31989934175544476602  testing_set_loss 3013.85014194163250067504
epoch 172  training_set_loss 2261.12647613136414292967  testing_set_loss 3013.79969972390153998276
epoch 173  training_set_loss 2260.92075853318328881869  testing_set_loss 3013.79001118379164836369
epoch 174  training_set_loss 2260.70179061288035882171  testing_set_loss 3013.75194516165356617421
epoch 175  training_set_loss 2260.46753451251925071119  testing_set_loss 3013.58927705619407788618
epoch 176  training_set_loss 2260.34879949885635141982  testing_set_loss 3013.56675459660982596688
epoch 177  training_set_loss 2260.20784943310718517751  testing_set_loss 3013.53644010123070984264
epoch 178  training_set_loss 2260.04670190779233962530  testing_set_loss 3013.61500881643905813689
epoch 179  training_set_loss 2259.87122630797830424854  testing_set_loss 3013.52091548790394881507
epoch 180  training_set_loss 2259.66631211416779478895  testing_set_loss 3013.53196059941228668322
epoch 181  training_set_loss 2259.55482992835914046736  testing_set_loss 3013.42250870059706358006
epoch 182  training_set_loss 2259.41997052793703915086  testing_set_loss 3013.29949485748102233629
epoch 183  training_set_loss 2259.25070007496879043174  testing_set_loss 3013.20877699593347642804
epoch 184  training_set_loss 2259.05859873010922456160  testing_set_loss 3013.08508290001145724091
epoch 185  training_set_loss 2258.82973405683787859743  testing_set_loss 3012.93552468145935563371
epoch 186  training_set_loss 2258.68918540623872104334  testing_set_loss 3012.96301220299346823595
epoch 187  training_set_loss 2258.51775263921081204899  testing_set_loss 3012.93322481619543395936
epoch 188  training_set_loss 2258.30727967437042025267  testing_set_loss 3013.15127355938693654025
epoch 189  training_set_loss 2258.03944725134169857483  testing_set_loss 3013.04145396652847921359
epoch 190  training_set_loss 2257.68616609349237478455  testing_set_loss 3013.12982112339750528918
epoch 191  training_set_loss 2257.46034075285024300683  testing_set_loss 3012.99962962255312959314
epoch 192  training_set_loss 2257.15907008393423893722  testing_set_loss 3012.83423069226319057634
epoch 193  training_set_loss 2256.77648501366820710246  testing_set_loss 3012.71958311572598177008
epoch 194  training_set_loss 2256.27110565383145512897  testing_set_loss 3012.43889024686995981028
epoch 195  training_set_loss 2255.93846263844443456037  testing_set_loss 3012.25364892078914635931
epoch 196  training_set_loss 2255.51314049417851492763  testing_set_loss 3011.93966801702435986954
epoch 197  training_set_loss 2255.00043007563408536953  testing_set_loss 3011.64840186353876561043
epoch 198  training_set_loss 2254.43875798875478722039  testing_set_loss 3011.55030040592555451440
epoch 199  training_set_loss 2253.85960421651361684781  testing_set_loss 3011.17563707083763802075
epoch 200  training_set_loss 2253.56716625433364242781  testing_set_loss 3010.97283298569118414889
epoch 201  training_set_loss 2253.25240764658246916952  testing_set_loss 3010.73205798353546924773
epoch 202  training_set_loss 2252.93633901834982680157  testing_set_loss 3010.56053442324400748475
epoch 203  training_set_loss 2252.56270363828934932826  testing_set_loss 3010.44656355847018858185
epoch 204  training_set_loss 2252.14761483239954031887  testing_set_loss 3010.24165709074941332801
epoch 205  training_set_loss 2251.89625909019150640233  testing_set_loss 3009.93956772724050097167
epoch 206  training_set_loss 2251.57841372704342575162  testing_set_loss 3009.64727121194027859019
epoch 207  training_set_loss 2251.20930818924580307794  testing_set_loss 3009.37502378053295615246
epoch 208  training_set_loss 2250.74874917374245342216  testing_set_loss 3009.04051903715662774630
epoch 209  training_set_loss 2250.25111894094106901321  testing_set_loss 3008.54055962216853004065
epoch 210  training_set_loss 2250.01109226185144507326  testing_set_loss 3008.56460758185585291358
epoch 211  training_set_loss 2249.79511078410860136501  testing_set_loss 3008.49223311114292300772
epoch 212  training_set_loss 2249.58813493222942270222  testing_set_loss 3008.58618964014112862060
epoch 213  training_set_loss 2249.39022419662069296464  testing_set_loss 3008.69910879157851013588
epoch 214  training_set_loss 2249.19370540684394654818  testing_set_loss 3008.70554023415388655849
epoch 215  training_set_loss 2249.09145927125746311503  testing_set_loss 3008.59857776678791196900
epoch 216  training_set_loss 2248.98446259582533457433  testing_set_loss 3008.65640623387344021467
epoch 217  training_set_loss 2248.87080749759888931294  testing_set_loss 3008.58598986091510596452
epoch 218  training_set_loss 2248.74803028645146696363  testing_set_loss 3008.51822758066555252299
epoch 219  training_set_loss 2248.68471277679236663971  testing_set_loss 3008.50032078277217806317
epoch 220  training_set_loss 2248.60031416400624948437  testing_set_loss 3008.33352259237290127203
epoch 221  training_set_loss 2248.51665883054874939262  testing_set_loss 3008.28745592488030524692
epoch 222  training_set_loss 2248.42173965974870952778  testing_set_loss 3007.99367362142447746010
epoch 223  training_set_loss 2248.31750414758562328643  testing_set_loss 3007.94521643124653564882
epoch 224  training_set_loss 2248.25871834164354368113  testing_set_loss 3007.85538966105241343030
epoch 225  training_set_loss 2248.19614291920333926100  testing_set_loss 3007.72222547603951170458
epoch 226  training_set_loss 2248.12470093220917988219  testing_set_loss 3007.70146219361231487710
epoch 227  training_set_loss 2248.03966602104082994629  testing_set_loss 3007.61382962590778333833
epoch 228  training_set_loss 2247.94778245628685908741  testing_set_loss 3007.58302947306083297008
epoch 229  training_set_loss 2247.89606358539958819165  testing_set_loss 3007.57112334393696073676
epoch 230  training_set_loss 2247.83672425787972315447  testing_set_loss 3007.47692072447307509719
epoch 231  training_set_loss 2247.76746014100490356213  testing_set_loss 3007.51502820368523316574
epoch 232  training_set_loss 2247.69363116796466783853  testing_set_loss 3007.45447827273437724216
epoch 233  training_set_loss 2247.60996585189104735036  testing_set_loss 3007.58288591387236010632
epoch 234  training_set_loss 2247.56449805929514695890  testing_set_loss 3007.54035017471369428677
epoch 235  training_set_loss 2247.51110930154891320853  testing_set_loss 3007.45487336132646305487
epoch 236  training_set_loss 2247.45427996434682427207  testing_set_loss 3007.22265090097107531619
epoch 237  training_set_loss 2247.38861767567505012266  testing_set_loss 3007.07067884437537941267
epoch 238  training_set_loss 2247.31317476850517778075  testing_set_loss 3006.87680436515211113147
epoch 239  training_set_loss 2247.26768611954776133643  testing_set_loss 3006.61016078558350272942
epoch 240  training_set_loss 2247.21760361652013671119  testing_set_loss 3006.34343918642252901918
epoch 241  training_set_loss 2247.15978732295525333029  testing_set_loss 3006.18232665863479269319
epoch 242  training_set_loss 2247.09516970298454907606  testing_set_loss 3006.05754363285450381227
epoch 243  training_set_loss 2247.05723527736654432374  testing_set_loss 3006.01190399158076616004
epoch 244  training_set_loss 2247.01325319000216040877  testing_set_loss 3005.97939706643410318065
epoch 245  training_set_loss 2246.96254907261845801258  testing_set_loss 3005.93080037895015266258
epoch 246  training_set_loss 2246.90363609324049321003  testing_set_loss 3005.85426708793829675415
epoch 247  training_set_loss 2246.83637975632336747367  testing_set_loss 3005.73504131696699914755
epoch 248  training_set_loss 2246.79308721551205962896  testing_set_loss 3005.57207166792886710027
epoch 249  training_set_loss 2246.74974598414291904191  testing_set_loss 3005.30041224409205824486
epoch 250  training_set_loss 2246.69792558866811305052  testing_set_loss 3005.23241327847472348367
epoch 251  training_set_loss 2246.63754292727026040666  testing_set_loss 3005.27857537583395242109
epoch 252  training_set_loss 2246.57301211000049079303  testing_set_loss 3005.18332511444123156252
epoch 253  training_set_loss 2246.53642604766901058611  testing_set_loss 3005.11934709124716391671
epoch 254  training_set_loss 2246.49654231827571493341  testing_set_loss 3005.05388657160119691980
epoch 255  training_set_loss 2246.45030851511546643451  testing_set_loss 3005.04427576267653421382
epoch 256  training_set_loss 2246.39682487589880111045  testing_set_loss 3004.95011691474564941018
epoch 257  training_set_loss 2246.34053791229734997614  testing_set_loss 3005.06105315299691937980
epoch 258  training_set_loss 2246.30604277982547500869  testing_set_loss 3004.94141492232438395149
epoch 259  training_set_loss 2246.26071457115267548943  testing_set_loss 3004.82467656441349390661
epoch 260  training_set_loss 2246.21155748464252610574  testing_set_loss 3004.54964375738472881494
epoch 261  training_set_loss 2246.15701667725215884275  testing_set_loss 3004.40814326817189794383
epoch 262  training_set_loss 2246.09276112536872460623  testing_set_loss 3004.26616076466598315164
epoch 263  training_set_loss 2246.05091742154763778672  testing_set_loss 3004.30737966051083276398
epoch 264  training_set_loss 2246.00533483856816019397  testing_set_loss 3004.18120957910559809534
epoch 265  training_set_loss 2245.94905593063458582037  testing_set_loss 3004.10415177346158088767
epoch 266  training_set_loss 2245.88427254064345106599  testing_set_loss 3003.89407543038032599725
epoch 267  training_set_loss 2245.84759383577193148085  testing_set_loss 3003.82157125392359375837
epoch 268  training_set_loss 2245.80753168205637848587  testing_set_loss 3003.82543080833875137614
epoch 269  training_set_loss 2245.75687496210957760923  testing_set_loss 3003.84365185927345009986
epoch 270  training_set_loss 2245.69742451542015260202  testing_set_loss 3003.82568908650637240498
epoch 271  training_set_loss 2245.62824731454975335510  testing_set_loss 3003.78740863196435384452
epoch 272  training_set_loss 2245.58964857353703337139  testing_set_loss 3003.63753931203154934337
epoch 273  training_set_loss 2245.54385695138989831321  testing_set_loss 3003.42735785485774613335
epoch 274  training_set_loss 2245.49269849834990964155  testing_set_loss 3003.13835435891860470292
epoch 275  training_set_loss 2245.42981593686408814392  testing_set_loss 3002.89610152788736741059
epoch 276  training_set_loss 2245.35522572162562937592  testing_set_loss 3002.67893074067615089007
epoch 277  training_set_loss 2245.31438471151523117442  testing_set_loss 3002.68166765145633689826
epoch 278  training_set_loss 2245.26294666320791293401  testing_set_loss 3002.57468714129117870471
epoch 279  training_set_loss 2245.19960168744910333771  testing_set_loss 3002.27689859178144615726
epoch 280  training_set_loss 2245.12769580897838750388  testing_set_loss 3002.09535944639219451346
epoch 281  training_set_loss 2245.03760278013896822813  testing_set_loss 3001.85271089123762067175
epoch 282  training_set_loss 2244.98567594143196402001  testing_set_loss 3001.67871486575495509896
epoch 283  training_set_loss 2244.92153223234799952479  testing_set_loss 3001.52824926863013388356
epoch 284  training_set_loss 2244.84581021584426707705  testing_set_loss 3001.25955686410998168867
epoch 285  training_set_loss 2244.75003226500666642096  testing_set_loss 3000.96231102546016700217
epoch 286  training_set_loss 2244.61634866622262052260  testing_set_loss 3000.68818228127111069625
epoch 287  training_set_loss 2244.54332783611471313634  testing_set_loss 3000.40998221120025846176
epoch 288  training_set_loss 2244.45272127174348497647  testing_set_loss 3000.26675724956976409885
epoch 289  training_set_loss 2244.34202942223510035546  testing_set_loss 2999.99886647027597064152
epoch 290  training_set_loss 2244.19716919189477266627  testing_set_loss 2999.65946826918843726162
epoch 291  training_set_loss 2244.10876139340052759508  testing_set_loss 2999.44880218933985815966
epoch 292  training_set_loss 2243.99360874377043728600  testing_set_loss 2999.13366018946726399008
epoch 293  training_set_loss 2243.84058097971546885674  testing_set_loss 2998.88634850185462710215
epoch 294  training_set_loss 2243.65543618976653306163  testing_set_loss 2998.45287134847876586718
epoch 295  training_set_loss 2243.37445819470076457947  testing_set_loss 2997.91909210144967801170
epoch 296  training_set_loss 2243.19183690090630989289  testing_set_loss 2997.60422976485324397800
epoch 297  training_set_loss 2242.94330170910006927443  testing_set_loss 2997.11922325848945547477
epoch 298  training_set_loss 2242.61012476207770305336  testing_set_loss 2996.53155548788254236570
epoch 299  training_set_loss 2242.13937610575840153615  testing_set_loss 2995.92631662140684056794
epoch 300  training_set_loss 2241.48106860898269587778  testing_set_loss 2995.02523537993920399458
epoch 301  training_set_loss 2241.05127111656702254550  testing_set_loss 2994.73036930405169186997
epoch 302  training_set_loss 2240.53747405723743213457  testing_set_loss 2994.26060918830989976414
epoch 303  training_set_loss 2239.93447473861579055665  testing_set_loss 2993.90217711061677619000
epoch 304  training_set_loss 2239.26701238057103182655  testing_set_loss 2993.44957752805930795148
epoch 305  training_set_loss 2238.60527471467230498092  testing_set_loss 2993.08147209721801118576
epoch 306  training_set_loss 2238.28139544289251716691  testing_set_loss 2992.97874884068778555957
epoch 307  training_set_loss 2237.89972764558342532837  testing_set_loss 2992.88856639347022792208
epoch 308  training_set_loss 2237.45073144072694049100  testing_set_loss 2992.87527600658677329193
epoch 309  training_set_loss 2236.88909426255759171909  testing_set_loss 2992.69749079943585456931
epoch 310  training_set_loss 2236.14673121620671736309  testing_set_loss 2992.64451074215367043507
epoch 311  training_set_loss 2235.70729929348453879356  testing_set_loss 2992.39045768755750032142
epoch 312  training_set_loss 2235.19455760612117956043  testing_set_loss 2992.22121979593339347048
epoch 313  training_set_loss 2234.63707912796871823957  testing_set_loss 2992.18378877430086504319
epoch 314  training_set_loss 2234.09344664551463210955  testing_set_loss 2991.93381149247579742223
epoch 315  training_set_loss 2233.80091709036878455663  testing_set_loss 2992.02041765748754187371
epoch 316  training_set_loss 2233.46144405177392400219  testing_set_loss 2991.99133998405977763468
epoch 317  training_set_loss 2233.05457973617012612522  testing_set_loss 2991.87708689885130297625
epoch 318  training_set_loss 2232.59870299540398264071  testing_set_loss 2991.85432409340182857704
epoch 319  training_set_loss 2232.02999279601044690935  testing_set_loss 2991.79502688243928787415
epoch 320  training_set_loss 2231.75322428491426762776  testing_set_loss 2991.48873792131507798331
epoch 321  training_set_loss 2231.38795787651906721294  testing_set_loss 2991.46343222819086804520
epoch 322  training_set_loss 2230.97228110553623992018  testing_set_loss 2991.23080264305735909147
epoch 323  training_set_loss 2230.48997281667698189267  testing_set_loss 2991.02838388313648465555
epoch 324  training_set_loss 2229.91048909341134276474  testing_set_loss 2990.54322392954782117158
epoch 325  training_set_loss 2229.62281180627815047046  testing_set_loss 2990.43930274070407904219
epoch 326  training_set_loss 2229.29841935101148919784  testing_set_loss 2990.31563001694485137705
epoch 327  training_set_loss 2228.91889424680584852467  testing_set_loss 2990.16768527803014876554
epoch 328  training_set_loss 2228.49029442050732541247  testing_set_loss 2989.90615623758458241355
epoch 329  training_set_loss 2228.00168080368302980787  testing_set_loss 2989.39887805028183720424
epoch 330  training_set_loss 2227.74175511340081357048  testing_set_loss 2989.44509125128251980641
epoch 331  training_set_loss 2227.43518276794429766596  testing_set_loss 2989.40702404760895660729
epoch 332  training_set_loss 2227.07241054533324131626  testing_set_loss 2989.38945551575852732640
epoch 333  training_set_loss 2226.58886948448753173579  testing_set_loss 2989.13619825328169099521
epoch 334  training_set_loss 2226.06682116198999210610  testing_set_loss 2989.16987963075962397852
epoch 335  training_set_loss 2225.68188743068367330125  testing_set_loss 2989.19144237921273088432
epoch 336  training_set_loss 2225.22216796822112883092  testing_set_loss 2989.15661793807566937176
epoch 337  training_set_loss 2224.53917754908025017357  testing_set_loss 2989.12696454870774687151
epoch 338  training_set_loss 2223.64720665249251396745  testing_set_loss 2989.32813302897511675837
epoch 339  training_set_loss 2223.08829284352623290033  testing_set_loss 2989.64802121845150395529
epoch 340  training_set_loss 2222.44715570754215150373  testing_set_loss 2989.99804957107198788435
epoch 341  training_set_loss 2221.74431006810618782765  testing_set_loss 2990.44147864274145831587
epoch 342  training_set_loss 2221.09272137955440484802  testing_set_loss 2990.95360994884276806260
epoch 343  training_set_loss 2220.60252838639962646994  testing_set_loss 2991.40132570453943117172
epoch 344  training_set_loss 2220.42390300708575523458  testing_set_loss 2991.50953388759990048129
epoch 345  training_set_loss 2220.23418992249435177655  testing_set_loss 2991.62315572550687647890
epoch 346  training_set_loss 2220.04572081378273651353  testing_set_loss 2991.66313034786617208738
epoch 347  training_set_loss 2219.86608469985185365658  testing_set_loss 2991.72380524820027858368
epoch 348  training_set_loss 2219.68614461173410745687  testing_set_loss 2991.69932643942183858599
epoch 349  training_set_loss 2219.58518486181810658309  testing_set_loss 2991.79778381344385707052
epoch 350  training_set_loss 2219.47833814793284545885  testing_set_loss 2991.77843921265593962744
epoch 351  training_set_loss 2219.35390933756161757628  testing_set_loss 2991.94550935267989189015
epoch 352  training_set_loss 2219.24083868764409999130  testing_set_loss 2991.98690998437768939766
epoch 353  training_set_loss 2219.10742403981657844270  testing_set_loss 2992.05715966979050790542
epoch 354  training_set_loss 2219.04484407585005101282  testing_set_loss 2991.96229144537346655852
epoch 355  training_set_loss 2218.96161357256551127648  testing_set_loss 2992.04671769068909270572
epoch 356  training_set_loss 2218.86993191647070489125  testing_set_loss 2992.03753458327901171288
epoch 357  training_set_loss 2218.77368609844779712148  testing_set_loss 2992.02270103728187677916
epoch 358  training_set_loss 2218.67014230714812583756  testing_set_loss 2991.94416030845241039060
epoch 359  training_set_loss 2218.61612858836906525539  testing_set_loss 2991.86041516112754834467
epoch 360  training_set_loss 2218.55245241892680496676  testing_set_loss 2991.76427264401445427211
epoch 361  training_set_loss 2218.48002291429702381720  testing_set_loss 2991.70391129643758176826
epoch 362  training_set_loss 2218.39607032013782372815  testing_set_loss 2991.52348402833786167321
epoch 363  training_set_loss 2218.34933849810386163881  testing_set_loss 2991.48675449825805117143
epoch 364  training_set_loss 2218.29375076715859904652  testing_set_loss 2991.40908431604702855111
epoch 365  training_set_loss 2218.23393906258388597053  testing_set_loss 2991.23577199452893182752
epoch 366  training_set_loss 2218.16456236081876340904  testing_set_loss 2991.15047270459535866394
epoch 367  training_set_loss 2218.08303044671583847958  testing_set_loss 2991.26489808841961348662
epoch 368  training_set_loss 2218.03955260451675712829  testing_set_loss 2991.14728955213058725349
epoch 369  training_set_loss 2217.99005966366530628875  testing_set_loss 2991.11765147899996009073
epoch 370  training_set_loss 2217.93791323555888084229  testing_set_loss 2991.01988943294236378279
epoch 371  training_set_loss 2217.87620355313765685423  testing_set_loss 2990.74460900301482979557
epoch 372  training_set_loss 2217.80607769529478900949  testing_set_loss 2990.51010683859658456640
epoch 373  training_set_loss 2217.76794270561413213727  testing_set_loss 2990.43790863177537175943
epoch 374  training_set_loss 2217.72272352478375978535  testing_set_loss 2990.28862190690279021510
epoch 375  training_set_loss 2217.66411301672678746399  testing_set_loss 2989.93792185678239547997
epoch 376  training_set_loss 2217.59868892603117274120  testing_set_loss 2989.69743499583410084597
epoch 377  training_set_loss 2217.51938778830253795604  testing_set_loss 2989.54520210449663863983
epoch 378  training_set_loss 2217.47318356301957464893  testing_set_loss 2989.26257801848896633601
epoch 379  training_set_loss 2217.42153060614600690315  testing_set_loss 2989.04610813923409295967
epoch 380  training_set_loss 2217.35859217971983525786  testing_set_loss 2988.82985912680760520743
epoch 381  training_set_loss 2217.28779268759853948723  testing_set_loss 2988.67711037479148217244
epoch 382  training_set_loss 2217.20346157098765615956  testing_set_loss 2988.48971682103729108348
epoch 383  training_set_loss 2217.15058117026455875020  testing_set_loss 2988.09894803355336989625
epoch 384  training_set_loss 2217.08718986113990467857  testing_set_loss 2987.84293037129828007892
epoch 385  training_set_loss 2217.01203921462683865684  testing_set_loss 2987.59688563536974470480
epoch 386  training_set_loss 2216.91425572306889080210  testing_set_loss 2987.27644189975580957253
epoch 387  training_set_loss 2216.85609431707598560024  testing_set_loss 2987.11994118570009959512
epoch 388  training_set_loss 2216.78708225483069327311  testing_set_loss 2986.88568014059956112760
epoch 389  training_set_loss 2216.69899082607162199565  testing_set_loss 2986.56672406270990904886
epoch 390  training_set_loss 2216.59210635471481509740  testing_set_loss 2986.41866663535938641871
epoch 391  training_set_loss 2216.45381936421381396940  testing_set_loss 2986.07425572367310451227
epoch 392  training_set_loss 2216.36699214072996255709  testing_set_loss 2985.98973261893206654349
epoch 393  training_set_loss 2216.25727770692537887953  testing_set_loss 2985.82665751607964921277
epoch 394  training_set_loss 2216.11191037711887474870  testing_set_loss 2985.54511805784022726584
epoch 395  training_set_loss 2215.92433669312094934867  testing_set_loss 2985.34425299865006309119
epoch 396  training_set_loss 2215.68337575251553062117  testing_set_loss 2985.01383474140629914473
epoch 397  training_set_loss 2215.53345256504371718620  testing_set_loss 2984.96942914816963821067
epoch 398  training_set_loss 2215.33693678285953865270  testing_set_loss 2984.79218906891219376121
epoch 399  training_set_loss 2215.09882124468003894435  testing_set_loss 2984.70038047247999202227
epoch 400  training_set_loss 2214.82433655044815168367  testing_set_loss 2984.50441106999414841994
epoch 401  training_set_loss 2214.53136998146646874375  testing_set_loss 2984.27219956285671287333
epoch 402  training_set_loss 2214.38065807982957267086  testing_set_loss 2984.05687530614477509516
epoch 403  training_set_loss 2214.21598480573447886854  testing_set_loss 2984.01503354754277097527
epoch 404  training_set_loss 2214.06269189000386177213  testing_set_loss 2983.98593996568661168567
epoch 405  training_set_loss 2213.90735271208859558101  testing_set_loss 2983.88946811432469985448
epoch 406  training_set_loss 2213.73995074244294301025  testing_set_loss 2983.56874882696320128161
epoch 407  training_set_loss 2213.65629004756465292303  testing_set_loss 2983.56464146004645954235
epoch 408  training_set_loss 2213.57040954693275125464  testing_set_loss 2983.46580283195089577930
epoch 409  training_set_loss 2213.47204351676009537186  testing_set_loss 2983.41236078983638435602
epoch 410  training_set_loss 2213.37003505771826894488  testing_set_loss 2983.30602419956903759157
epoch 411  training_set_loss 2213.31859484161577711347  testing_set_loss 2983.28679922181208894472
epoch 412  training_set_loss 2213.25927542483668730711  testing_set_loss 2983.16338242252277268562
epoch 413  training_set_loss 2213.18647171567135956138  testing_set_loss 2983.11681014801979472395
epoch 414  training_set_loss 2213.10771216602461208822  testing_set_loss 2983.01998484494697549962
epoch 415  training_set_loss 2213.01805755775376383099  testing_set_loss 2982.89308476435917327763
epoch 416  training_set_loss 2212.96927030465985808405  testing_set_loss 2982.71010222342829365516
epoch 417  training_set_loss 2212.91373971202710890793  testing_set_loss 2982.43859570550557691604
epoch 418  training_set_loss 2212.84755856637366377981  testing_set_loss 2982.32391174514623344294
epoch 419  training_set_loss 2212.76549016444641893031  testing_set_loss 2981.97753081482323977980
epoch 420  training_set_loss 2212.68865984830881643575  testing_set_loss 2982.05784018585563899251
epoch 421  training_set_loss 2212.64203491372836651863  testing_set_loss 2982.03379542672291790950
epoch 422  training_set_loss 2212.58883073941706243204  testing_set_loss 2982.12518967159940075362
epoch 423  training_set_loss 2212.51779611534766445402  testing_set_loss 2981.99458907519056083402
epoch 424  training_set_loss 2212.43927544738426149706  testing_set_loss 2981.86191332715407042997
epoch 425  training_set_loss 2212.34290133565446012653  testing_set_loss 2981.66219170425438278471
epoch 426  training_set_loss 2212.28259162137828752748  testing_set_loss 2981.61466105888757738285
epoch 427  training_set_loss 2212.20427910840544427629  testing_set_loss 2981.62662408985988804488
epoch 428  training_set_loss 2212.11261655625412458903  testing_set_loss 2981.45921684141922014533
epoch 429  training_set_loss 2211.99314676546237024013  testing_set_loss 2981.40272839326189568965
epoch 430  training_set_loss 2211.84140467166571397684  testing_set_loss 2981.34536696558097901288
epoch 431  training_set_loss 2211.74490314376043897937  testing_set_loss 2981.29570471845045176451
epoch 432  training_set_loss 2211.62105251273987960303  testing_set_loss 2981.26552928739965864224
epoch 433  training_set_loss 2211.45874135025633222540  testing_set_loss 2981.19399402413273492130
epoch 434  training_set_loss 2211.23890382280978883500  testing_set_loss 2981.00599516760394180892
epoch 435  training_set_loss 2211.08814071342567331158  testing_set_loss 2980.89555912926425662590
epoch 436  training_set_loss 2210.88187663118242198834  testing_set_loss 2980.73362884447578835534
epoch 437  training_set_loss 2210.61540403452363534598  testing_set_loss 2980.62083594764453664538
epoch 438  training_set_loss 2210.28321573661150978296  testing_set_loss 2980.43063847114945019712
epoch 439  training_set_loss 2209.87386956103409829666  testing_set_loss 2980.00815488790431118105
epoch 440  training_set_loss 2209.68120066866003980977  testing_set_loss 2979.86634792847098651691
epoch 441  training_set_loss 2209.47317723397554800613  testing_set_loss 2979.65494188219281568308
epoch 442  training_set_loss 2209.25929155936546521843  testing_set_loss 2979.43039423086020178744
epoch 443  training_set_loss 2209.05245480908706667833  testing_set_loss 2979.18023211739227917860
epoch 444  training_set_loss 2208.85912918861413345439  testing_set_loss 2978.78790726269198785303
epoch 445  training_set_loss 2208.75477748245475595468  testing_set_loss 2978.54503836959338514134
epoch 446  training_set_loss 2208.64455037606239784509  testing_set_loss 2978.30889304850325061125
epoch 447  training_set_loss 2208.51707549659795404295  testing_set_loss 2977.97689607129677824560
epoch 448  training_set_loss 2208.37754827418075365131  testing_set_loss 2977.76180236373329535127
epoch 449  training_set_loss 2208.21329291993424703833  testing_set_loss 2977.21203678356278032879
epoch 450  training_set_loss 2208.11637496625189669430  testing_set_loss 2977.04235339210981692304
epoch 451  training_set_loss 2208.00009747489320943714  testing_set_loss 2976.75758761295037402306
epoch 452  training_set_loss 2207.85207176550920848968  testing_set_loss 2976.41388799012975141522
epoch 453  training_set_loss 2207.66869556972096688696  testing_set_loss 2975.73325942905967167462
epoch 454  training_set_loss 2207.42858489292893864331  testing_set_loss 2975.15938630962773459032
epoch 455  training_set_loss 2207.27423440792745168437  testing_set_loss 2974.72336938355056190630
epoch 456  training_set_loss 2207.08179709581827410148  testing_set_loss 2974.10471094873582842411
epoch 457  training_set_loss 2206.83045833669439161895  testing_set_loss 2973.47797687936417787569
epoch 458  training_set_loss 2206.52895820804587856401  testing_set_loss 2972.85181068301835694001
epoch 459  training_set_loss 2206.32755906950387725374  testing_set_loss 2972.24778380062934957095
epoch 460  training_set_loss 2206.07487538866689646966  testing_set_loss 2971.79544959174836549209
epoch 461  training_set_loss 2205.77203457086761773098  testing_set_loss 2971.13810679972993966658
epoch 462  training_set_loss 2205.41371369873650110094  testing_set_loss 2970.62944599036563886330
epoch 463  training_set_loss 2204.99889451328454015311  testing_set_loss 2969.99474138030700487434
epoch 464  training_set_loss 2204.77437774473219178617  testing_set_loss 2969.63383307304502523039
epoch 465  training_set_loss 2204.51874716586098656990  testing_set_loss 2969.25651098753951373510
epoch 466  training_set_loss 2204.23789965810374269495  testing_set_loss 2968.91530336046071170131
epoch 467  training_set_loss 2203.93431107886408426566  testing_set_loss 2968.46286860381314909318
epoch 468  training_set_loss 2203.59181449149355103145  testing_set_loss 2967.97450775740890094312
epoch 469  training_set_loss 2203.41176837032162438845  testing_set_loss 2967.80959972264645330142
epoch 470  training_set_loss 2203.18805466319963670685  testing_set_loss 2967.51668525342211069074
epoch 471  training_set_loss 2202.93103764883289841237  testing_set_loss 2967.28363011838837337564
epoch 472  training_set_loss 2202.62364491132802868378  testing_set_loss 2967.17948086620162939653
epoch 473  training_set_loss 2202.28346509646189588238  testing_set_loss 2966.75137025738740703673
epoch 474  training_set_loss 2202.09740656812164161238  testing_set_loss 2966.58968957309298275504
epoch 475  training_set_loss 2201.87869468372946357704  testing_set_loss 2966.53782116198499352322
epoch 476  training_set_loss 2201.61121334861672949046  testing_set_loss 2966.40252802343002258567
epoch 477  training_set_loss 2201.28751292895185542875  testing_set_loss 2966.37319600266573615954
epoch 478  training_set_loss 2200.90852140766037337016  testing_set_loss 2966.37334830094960125280
epoch 479  training_set_loss 2200.68518269445394253125  testing_set_loss 2966.28151422107157486607
epoch 480  training_set_loss 2200.40890521307983362931  testing_set_loss 2966.24435639974035439081
epoch 481  training_set_loss 2200.06591170871433860157  testing_set_loss 2966.21971714785377116641
epoch 482  training_set_loss 2199.62161054616944966256  testing_set_loss 2966.22353704440092769801
epoch 483  training_set_loss 2199.35795135646276321495  testing_set_loss 2966.18804989994669085718
epoch 484  training_set_loss 2199.03264459571346378652  testing_set_loss 2966.17461203041148110060
epoch 485  training_set_loss 2198.61464190635297200060  testing_set_loss 2966.11936501197078541736
epoch 486  training_set_loss 2198.08455733034543300164  testing_set_loss 2965.99833503716399718542
epoch 487  training_set_loss 2197.41859275350907410029  testing_set_loss 2965.89419174459635542007
epoch 488  training_set_loss 2197.03655924970871637925  testing_set_loss 2965.90247615132057035225
epoch 489  training_set_loss 2196.57713566676238770015  testing_set_loss 2965.93729779985415007104
epoch 490  training_set_loss 2196.02463363317156108678  testing_set_loss 2965.93725487965457432438
epoch 491  training_set_loss 2195.36214854280842700973  testing_set_loss 2965.94898875970329754637
epoch 492  training_set_loss 2194.52885110861689099693  testing_set_loss 2965.99029603812186905998
epoch 493  training_set_loss 2194.04609224259411348612  testing_set_loss 2966.03164829203024055460
epoch 494  training_set_loss 2193.47736633568501929403  testing_set_loss 2966.04992630267361164442
epoch 495  training_set_loss 2192.83911983741745643783  testing_set_loss 2966.12609803910936534521
epoch 496  training_set_loss 2192.08058450420321605634  testing_set_loss 2966.22397582332541787764
epoch 497  training_set_loss 2191.22775624885298384470  testing_set_loss 2966.13340855240585369756
epoch 498  training_set_loss 2190.75707332398587823263  testing_set_loss 2966.21224672919606746291
epoch 499  training_set_loss 2190.18091374829418782610  testing_set_loss 2966.24158465876053014654
epoch 500  training_set_loss 2189.44588282170388993109  testing_set_loss 2966.36286128890333202435
epoch 501  training_set_loss 2188.51393400195911453920  testing_set_loss 2966.31296233157172537176
epoch 502  training_set_loss 2187.37313607351143218693  testing_set_loss 2966.43313357261467899662
epoch 503  training_set_loss 2186.76244702221492843819  testing_set_loss 2966.47374441154215674032
epoch 504  training_set_loss 2186.05933603839230272570  testing_set_loss 2966.68769457411644907552
epoch 505  training_set_loss 2185.32947979162599949632  testing_set_loss 2966.91840595810754166450
epoch 506  training_set_loss 2184.61515519585191213991  testing_set_loss 2967.06054677354950399604
epoch 507  training_set_loss 2184.29691261972266147495  testing_set_loss 2966.97015404230478452519
epoch 508  training_set_loss 2183.94665343670749280136  testing_set_loss 2967.07720913879347790498
epoch 509  training_set_loss 2183.58897787833348047570  testing_set_loss 2967.10013576170422311407
epoch 510  training_set_loss 2183.22461010863844421692  testing_set_loss 2966.78567680738979106536
epoch 511  training_set_loss 2182.83745774415501728072  testing_set_loss 2966.91605922267945061321
epoch 512  training_set_loss 2182.65995754929281247314  testing_set_loss 2966.60936040535443680710
epoch 513  training_set_loss 2182.45691622850608837325  testing_set_loss 2966.37708474373675926472
epoch 514  training_set_loss 2182.23609928628593479516  testing_set_loss 2966.21458968236083819647
epoch 515  training_set_loss 2182.00132634995952685131  testing_set_loss 2965.81639119578949248535
epoch 516  training_set_loss 2181.74247746414675930282  testing_set_loss 2965.37788587264049056103
epoch 517  training_set_loss 2181.60916959588212193921  testing_set_loss 2965.25370241789596548188
epoch 518  training_set_loss 2181.44993368588347948389  testing_set_loss 2965.07511104609693575185
epoch 519  training_set_loss 2181.28668043959623901173  testing_set_loss 2964.82924145312608743552
epoch 520  training_set_loss 2181.09656513332856775378  testing_set_loss 2964.61031575841707308427
epoch 521  training_set_loss 2180.89729945041244718595  testing_set_loss 2964.26576328077499056235
epoch 522  training_set_loss 2180.79254942720399412792  testing_set_loss 2964.20112307199224233045
epoch 523  training_set_loss 2180.66358769034559372813  testing_set_loss 2963.99583138046500607743
epoch 524  training_set_loss 2180.51925020555472656270  testing_set_loss 2963.69551069639328488847
epoch 525  training_set_loss 2180.35261969281737037818  testing_set_loss 2963.37877106145197103615
epoch 526  training_set_loss 2180.15799989868128250237  testing_set_loss 2963.04538800067393822246
epoch 527  training_set_loss 2180.04395911443134536967  testing_set_loss 2962.93390377972536953166
epoch 528  training_set_loss 2179.89887708135211141780  testing_set_loss 2962.77771854952698049601
epoch 529  training_set_loss 2179.73838013009708447498  testing_set_loss 2962.49149509102016963880
epoch 530  training_set_loss 2179.53956196324952543364  testing_set_loss 2962.01627941581227787538
epoch 531  training_set_loss 2179.43416810461485511041  testing_set_loss 2961.86479330155270872638
epoch 532  training_set_loss 2179.29511866249595186673  testing_set_loss 2961.51236236209115304518
epoch 533  training_set_loss 2179.13547327889409643831  testing_set_loss 2961.18816724068256007740
epoch 534  training_set_loss 2178.93161463231808738783  testing_set_loss 2960.59505142612670169910
epoch 535  training_set_loss 2178.66653819165048844297  testing_set_loss 2959.78076085002476247610
epoch 536  training_set_loss 2178.48441856083991297055  testing_set_loss 2959.30154228638411950669
epoch 537  training_set_loss 2178.26204920353575289482  testing_set_loss 2958.70535491835516950232
epoch 538  training_set_loss 2177.94287702021711083944  testing_set_loss 2957.78457033653648977634
epoch 539  training_set_loss 2177.51665636282768900855  testing_set_loss 2956.51190895413674297743
epoch 540  training_set_loss 2176.95980124595007509924  testing_set_loss 2954.97658349379707942717
epoch 541  training_set_loss 2176.61237937601845260360  testing_set_loss 2954.19091187971116596600
epoch 542  training_set_loss 2176.23176351517531657009  testing_set_loss 2953.17727195748193480540
epoch 543  training_set_loss 2175.83687514142911823001  testing_set_loss 2952.33051334465790205286
epoch 544  training_set_loss 2175.45793498225884832209  testing_set_loss 2951.21049922709880775074
epoch 545  training_set_loss 2175.06439642731538697262  testing_set_loss 2950.29472503719580345205
epoch 546  training_set_loss 2174.89031060072102263803  testing_set_loss 2950.24202702442335066735
epoch 547  training_set_loss 2174.66595159114831403713  testing_set_loss 2949.92177305278710264247
epoch 548  training_set_loss 2174.39697103645130482619  testing_set_loss 2949.56633858501663780771
epoch 549  training_set_loss 2174.07632713151770076365  testing_set_loss 2949.18072716849064818234
epoch 550  training_set_loss 2173.69243913222635455895  testing_set_loss 2948.67976274136526626535
epoch 551  training_set_loss 2173.45744695052189854323  testing_set_loss 2948.36956074244835690479
epoch 552  training_set_loss 2173.16054715900736482581  testing_set_loss 2947.99401494198855289142
epoch 553  training_set_loss 2172.80741685481734748464  testing_set_loss 2947.76296651282427774277
epoch 554  training_set_loss 2172.40761434669002483133  testing_set_loss 2947.41627636643579535303
epoch 555  training_set_loss 2171.92761024124502000632  testing_set_loss 2947.18138949094782219618
epoch 556  training_set_loss 2171.63641029026030082605  testing_set_loss 2947.12066794169413697091
epoch 557  training_set_loss 2171.30379708755708634271  testing_set_loss 2946.90619343923162887222
epoch 558  training_set_loss 2170.88943380057935428340  testing_set_loss 2946.76408076348207032424
epoch 559  training_set_loss 2170.38735514945574323065  testing_set_loss 2946.29224995139111342723
epoch 560  training_set_loss 2170.12265646948708308628  testing_set_loss 2946.02425490905761762406
epoch 561  training_set_loss 2169.80165617856619064696  testing_set_loss 2945.86677642659651610302
epoch 562  training_set_loss 2169.43603974367852060823  testing_set_loss 2945.84839448210641421610
epoch 563  training_set_loss 2169.04283722431455316837  testing_set_loss 2945.71591282795361621538
epoch 564  training_set_loss 2168.63710031226446517394  testing_set_loss 2945.53042424961176948273
epoch 565  training_set_loss 2168.42584196355346648488  testing_set_loss 2945.47453921809710664093
epoch 566  training_set_loss 2168.18382982705179529148  testing_set_loss 2945.13405007664687218494
epoch 567  training_set_loss 2167.91675010127482892131  testing_set_loss 2944.83163492082803713856
epoch 568  training_set_loss 2167.61233092778957143310  testing_set_loss 2944.60488271272424753988
epoch 569  training_set_loss 2167.26941778382570191752  testing_set_loss 2944.19888056368381512584
epoch 570  training_set_loss 2167.06786065218193471082  testing_set_loss 2944.00176928950259025441
epoch 571  training_set_loss 2166.82644803725816018414  testing_set_loss 2943.81190985957846351084
epoch 572  training_set_loss 2166.53327381290455377894  testing_set_loss 2943.53007667438714634045
epoch 573  training_set_loss 2166.17619619649849482812  testing_set_loss 2943.35579512719505146379
epoch 574  training_set_loss 2165.76325991233397871838  testing_set_loss 2943.16015184072512056446
epoch 575  training_set_loss 2165.52610441981869371375  testing_set_loss 2942.91259649742733017774
epoch 576  training_set_loss 2165.25581700235534299281  testing_set_loss 2942.68825307703991711605
epoch 577  training_set_loss 2164.96867462370710200048  testing_set_loss 2942.52742967278891228489
epoch 578  training_set_loss 2164.66340587442573450971  testing_set_loss 2942.25490816527508286526
epoch 579  training_set_loss 2164.34857508537743342458  testing_set_loss 2942.01409777551998558920
epoch 580  training_set_loss 2164.19300859667328040814  testing_set_loss 2941.74262651118488065549
epoch 581  training_set_loss 2164.01633830700075122877  testing_set_loss 2941.33991431672438920941
epoch 582  training_set_loss 2163.82869842080663147499  testing_set_loss 2941.21993548109048788319
epoch 583  training_set_loss 2163.61461967750119583798  testing_set_loss 2940.74525284103037847672
epoch 584  training_set_loss 2163.49269507346753016463  testing_set_loss 2940.45322898464655736461
epoch 585  training_set_loss 2163.35186727208065349259  testing_set_loss 2940.16360198894608402043
epoch 586  training_set_loss 2163.18464179958482418442  testing_set_loss 2939.85843409597009667777
epoch 587  training_set_loss 2162.99258102369321932201  testing_set_loss 2939.49510474994531250559
epoch 588  training_set_loss 2162.75034720437315627350  testing_set_loss 2939.14219315023638046114
epoch 589  training_set_loss 2162.60607855543685218436  testing_set_loss 2938.85249153374616071233
epoch 590  training_set_loss 2162.42865564621024532244  testing_set_loss 2938.56201594215599470772
epoch 591  training_set_loss 2162.20455675297398556722  testing_set_loss 2938.30485882312314060982
epoch 592  training_set_loss 2161.93202236273737071315  testing_set_loss 2937.85098464566408438259
epoch 593  training_set_loss 2161.60817138400489056949  testing_set_loss 2937.25380379295438615372
epoch 594  training_set_loss 2161.43969589641028505866  testing_set_loss 2936.98937681504594365833
epoch 595  training_set_loss 2161.21211780053454276640  testing_set_loss 2936.65181424398360832129
epoch 596  training_set_loss 2160.94561900158078060485  testing_set_loss 2936.19446452164174843347
epoch 597  training_set_loss 2160.63525878568225380150  testing_set_loss 2935.63048027075410573161
epoch 598  training_set_loss 2160.28680897882895806106  testing_set_loss 2934.87455219376897730399
epoch 599  training_set_loss 2160.09633355900359674706  testing_set_loss 2934.73416987425434854231
epoch 600  training_set_loss 2159.88836888632658883580  testing_set_loss 2934.51444926789872624795
epoch 601  training_set_loss 2159.63737313428237030166  testing_set_loss 2934.20031996184570743935
epoch 602  training_set_loss 2159.35999535345263211639  testing_set_loss 2933.92449415015380509431
epoch 603  training_set_loss 2159.04547265703376979218  testing_set_loss 2933.55260426777840621071
epoch 604  training_set_loss 2158.86420875470867031254  testing_set_loss 2933.48212540242866452900
epoch 605  training_set_loss 2158.64893170494087826228  testing_set_loss 2933.36260662750964911538
epoch 606  training_set_loss 2158.37292707127608082374  testing_set_loss 2932.86590314863906314713
epoch 607  training_set_loss 2158.01029428731726511614  testing_set_loss 2932.60887312017894146265
epoch 608  training_set_loss 2157.78541197607410140336  testing_set_loss 2932.45156306210219554487
epoch 609  training_set_loss 2157.48372546565178708988  testing_set_loss 2932.47445191258111663046
epoch 610  training_set_loss 2157.06730241248214952066  testing_set_loss 2932.33505896135193324881
epoch 611  training_set_loss 2156.51726727255254445481  testing_set_loss 2932.09729294189719439601
epoch 612  training_set_loss 2155.93618677601443778258  testing_set_loss 2931.79857693456415290711
epoch 613  training_set_loss 2155.61111355761522645480  testing_set_loss 2931.62978000471048289910
epoch 614  training_set_loss 2155.20696307456910290057  testing_set_loss 2931.29182470594605547376
epoch 615  training_set_loss 2154.78436693552612268832  testing_set_loss 2931.11978942542373260949
epoch 616  training_set_loss 2154.31061571576037749765  testing_set_loss 2930.91767051643319064169
epoch 617  training_set_loss 2153.75812013614222450997  testing_set_loss 2930.79182565984228858724
epoch 618  training_set_loss 2153.41796976239174910006  testing_set_loss 2930.55403311601958193933
epoch 619  training_set_loss 2153.02023481719606934348  testing_set_loss 2930.40251513429029728286
epoch 620  training_set_loss 2152.54168281409738483489  testing_set_loss 2930.27088247720848812605
epoch 621  training_set_loss 2151.97490771659659003490  testing_set_loss 2930.07590634023836173583
epoch 622  training_set_loss 2151.33412026822634288692  testing_set_loss 2930.05715603754697440309
epoch 623  training_set_loss 2150.96111831417283610790  testing_set_loss 2929.86002968319189676549
epoch 624  training_set_loss 2150.56443728384147107136  testing_set_loss 2929.71174739812113330117
epoch 625  training_set_loss 2150.12610331726864387747  testing_set_loss 2929.51515779083001689287
epoch 626  training_set_loss 2149.71259604152737665572  testing_set_loss 2929.38831720040207073907
epoch 627  training_set_loss 2149.26798350709441365325  testing_set_loss 2929.45244752502730989363
epoch 628  training_set_loss 2149.07740770937425622833  testing_set_loss 2929.46263584950793301687
epoch 629  training_set_loss 2148.86253605172350944486  testing_set_loss 2929.35478142190049766214
epoch 630  training_set_loss 2148.62965953857610656996  testing_set_loss 2929.60342783847681857878
epoch 631  training_set_loss 2148.39013020541915466310  testing_set_loss 2929.50021508298596017994
epoch 632  training_set_loss 2148.25402335738635883899  testing_set_loss 2929.48418275768335661269
epoch 633  training_set_loss 2148.10514583048507120111  testing_set_loss 2929.42174187366708792979
epoch 634  training_set_loss 2147.93762143933827246656  testing_set_loss 2929.44083178854316429351
epoch 635  training_set_loss 2147.74922508351164651685  testing_set_loss 2929.49045371166948825703
epoch 636  training_set_loss 2147.53558058914768480463  testing_set_loss 2929.40555748675615177490
epoch 637  training_set_loss 2147.41240807689791836310  testing_set_loss 2929.37999741468502179487
epoch 638  training_set_loss 2147.26478721250623493688  testing_set_loss 2929.28433666160026405123
epoch 639  training_set_loss 2147.08810853091972603579  testing_set_loss 2929.09918304825578161399
epoch 640  training_set_loss 2146.86970959882637544069  testing_set_loss 2928.98204584025734220631
epoch 641  training_set_loss 2146.59520348095020381152  testing_set_loss 2928.90382164863558500656
epoch 642  training_set_loss 2146.41941792840589187108  testing_set_loss 2928.75323030754680075916
epoch 643  training_set_loss 2146.19870073868423787644  testing_set_loss 2928.56247032261808271869
epoch 644  training_set_loss 2145.91009967971103833406  testing_set_loss 2928.50108176186950004194
epoch 645  training_set_loss 2145.52438959675555452122  testing_set_loss 2928.14363837630389753031
epoch 646  training_set_loss 2145.01438230131179807358  testing_set_loss 2927.58415281655061335186
epoch 647  training_set_loss 2144.67937304918177687796  testing_set_loss 2927.42786055317401405773
epoch 648  training_set_loss 2144.23729417089680282515  testing_set_loss 2927.30975344262378712301
epoch 649  training_set_loss 2143.68592422307483502664  testing_set_loss 2926.87031485968873312231
epoch 650  training_set_loss 2143.11689142776049266104  testing_set_loss 2926.51658830064525318448
epoch 651  training_set_loss 2142.45358460629540786613  testing_set_loss 2926.40479610459078685381
epoch 652  training_set_loss 2142.11800213870537845651  testing_set_loss 2926.29427215676514606457
epoch 653  training_set_loss 2141.73378195721443262300  testing_set_loss 2926.08143629115102157812
epoch 654  training_set_loss 2141.26957757374930224614  testing_set_loss 2925.81684874914253668976
epoch 655  training_set_loss 2140.70829226853220461635  testing_set_loss 2925.35420115422357412172
epoch 656  training_set_loss 2140.38550150148648754111  testing_set_loss 2925.14427242332703826833
epoch 657  training_set_loss 2139.98488789640123286517  testing_set_loss 2924.84085178980512864655
epoch 658  training_set_loss 2139.50926323389057870372  testing_set_loss 2924.18667875181154158781
epoch 659  training_set_loss 2138.90891530479848370305  testing_set_loss 2923.81943717448439201689
epoch 660  training_set_loss 2138.23980635543694006628  testing_set_loss 2923.08171645607444588677
epoch 661  training_set_loss 2137.86538938084049732424  testing_set_loss 2922.82440330705321684945
epoch 662  training_set_loss 2137.39881861017920527956  testing_set_loss 2922.37237982799888413865
epoch 663  training_set_loss 2136.83283729084814694943  testing_set_loss 2921.87011450194040662609
epoch 664  training_set_loss 2136.14455845404609135585  testing_set_loss 2921.27038182517480890965
epoch 665  training_set_loss 2135.29729069448831069167  testing_set_loss 2920.46779154529258448747
epoch 666  training_set_loss 2134.80414104166584365885  testing_set_loss 2920.06067641087474839878
epoch 667  training_set_loss 2134.21259138447157965857  testing_set_loss 2919.69703749701420747442
epoch 668  training_set_loss 2133.49720441452063823817  testing_set_loss 2918.90049113720169771113
epoch 669  training_set_loss 2132.67078138566012057709  testing_set_loss 2918.22053044713675262756
epoch 670  training_set_loss 2131.66677805783956500818  testing_set_loss 2917.41830104091104658437
epoch 671  training_set_loss 2131.14187779699341263040  testing_set_loss 2916.93162302130212992779
epoch 672  training_set_loss 2130.53199291237933721277  testing_set_loss 2916.39528075587168132188
epoch 673  training_set_loss 2129.83259120311868173303  testing_set_loss 2915.31853633448508844594
epoch 674  training_set_loss 2129.01309728347905547707  testing_set_loss 2914.65030023101689948817
epoch 675  training_set_loss 2128.07015592585685226368  testing_set_loss 2913.58863936222905977047
epoch 676  training_set_loss 2127.57787230673829981242  testing_set_loss 2912.94602865949445913429
epoch 677  training_set_loss 2127.03098460400360636413  testing_set_loss 2912.26055103915541621973
epoch 678  training_set_loss 2126.43041079021168116014  testing_set_loss 2911.76859415878197978600
epoch 679  training_set_loss 2125.78581334547652659239  testing_set_loss 2910.96766305398432450602
epoch 680  training_set_loss 2125.46687344413248865749  testing_set_loss 2910.50673129753386092489
epoch 681  training_set_loss 2125.09116350160820729798  testing_set_loss 2910.07471009037590192747
epoch 682  training_set_loss 2124.66470757247998335515  testing_set_loss 2909.50655882644332450582
epoch 683  training_set_loss 2124.16647916926240213797  testing_set_loss 2908.83934774537647172110
epoch 684  training_set_loss 2123.61208155539588915417  testing_set_loss 2908.06763838821962053771
epoch 685  training_set_loss 2123.30088739078428261564  testing_set_loss 2907.76417236990437231725
epoch 686  training_set_loss 2122.92904560666966062854  testing_set_loss 2907.15756583409211089020
epoch 687  training_set_loss 2122.51201808831910966546  testing_set_loss 2906.85140861763011344010
epoch 688  training_set_loss 2122.04443284520993984188  testing_set_loss 2906.26688830733155555208
epoch 689  training_set_loss 2121.52153289576335737365  testing_set_loss 2905.47157216602181506460
epoch 690  training_set_loss 2121.27418609352343992214  testing_set_loss 2905.07745076070477807662
epoch 691  training_set_loss 2120.99259334524685982615  testing_set_loss 2904.68748433428072530660
epoch 692  training_set_loss 2120.68459743007861106889  testing_set_loss 2904.36634978550591767998
epoch 693  training_set_loss 2120.36583816109350664192  testing_set_loss 2903.95339001213324081618
epoch 694  training_set_loss 2120.05244892725477257045  testing_set_loss 2903.84571691980454488657
epoch 695  training_set_loss 2119.88584470925525238272  testing_set_loss 2903.53747112758128423593
epoch 696  training_set_loss 2119.70662157262722757878  testing_set_loss 2903.26442050076093437383
epoch 697  training_set_loss 2119.50809535330381550011  testing_set_loss 2902.98232595267654687632
epoch 698  training_set_loss 2119.29055628451169468462  testing_set_loss 2902.68535028987889745622
epoch 699  training_set_loss 2119.06186954316399351228  testing_set_loss 2902.42445917449867920368
epoch 700  training_set_loss 2118.94658299814591373433  testing_set_loss 2902.27447231595806442783
epoch 701  training_set_loss 2118.81472042219184004352  testing_set_loss 2901.94493406734090967802
epoch 702  training_set_loss 2118.67598733162049029488  testing_set_loss 2901.89917691562186519150
epoch 703  training_set_loss 2118.52080259852345989202  testing_set_loss 2901.76953966770497572725
epoch 704  training_set_loss 2118.43375937285645704833  testing_set_loss 2901.62360930825889226981
epoch 705  training_set_loss 2118.33854043005749190343  testing_set_loss 2901.48164823013485147385
epoch 706  training_set_loss 2118.23040433652840874856  testing_set_loss 2901.39415986024687299505
epoch 707  training_set_loss 2118.11039405468136465061  testing_set_loss 2901.56894680908771988470
epoch 708  training_set_loss 2117.96696193552043041564  testing_set_loss 2901.36376087483404262457
epoch 709  training_set_loss 2117.88567387035300271236  testing_set_loss 2901.23520618777592972037
epoch 710  training_set_loss 2117.79422905977207847172  testing_set_loss 2901.21959039217881581862
epoch 711  training_set_loss 2117.69688830235963905579  testing_set_loss 2901.13938448111821344355
epoch 712  training_set_loss 2117.58803652561618946493  testing_set_loss 2901.05833508370778872631
epoch 713  training_set_loss 2117.47708355195845797425  testing_set_loss 2901.02460476303986069979
epoch 714  training_set_loss 2117.41470778600387347979  testing_set_loss 2900.91773301005014218390
epoch 715  training_set_loss 2117.34596115478143474320  testing_set_loss 2900.89310781485437473748
epoch 716  training_set_loss 2117.26538124657281514374  testing_set_loss 2900.80247423536820861045
epoch 717  training_set_loss 2117.16646122020074471948  testing_set_loss 2900.61948532434780645417
epoch 718  training_set_loss 2117.06095108131603410584  testing_set_loss 2900.54966983270878699841
epoch 719  training_set_loss 2117.00105820778162524221  testing_set_loss 2900.51786705129370602663
epoch 720  training_set_loss 2116.93022498807158626732  testing_set_loss 2900.48417451088016605354
epoch 721  training_set_loss 2116.85230493441440557945  testing_set_loss 2900.50961119788280484499
epoch 722  training_set_loss 2116.76361851282217685366  testing_set_loss 2900.46317943651411042083
epoch 723  training_set_loss 2116.66263876344737582258  testing_set_loss 2900.34547898845585223171
epoch 724  training_set_loss 2116.60093930664061190328  testing_set_loss 2900.33941869936961666099
epoch 725  training_set_loss 2116.53095560670499253320  testing_set_loss 2900.37688078502787902835
epoch 726  training_set_loss 2116.44922059723103302531  testing_set_loss 2900.40698477703745084000
epoch 727  training_set_loss 2116.36002243388975330163  testing_set_loss 2900.52978048112663600477
epoch 728  training_set_loss 2116.31184727872914663749  testing_set_loss 2900.61497852473803504836
epoch 729  training_set_loss 2116.25184542367696849396  testing_set_loss 2900.63485487331581680337
epoch 730  training_set_loss 2116.18050396646094668540  testing_set_loss 2900.62007411303466142272
epoch 731  training_set_loss 2116.09644551102655896102  testing_set_loss 2900.78652257705698502832
epoch 732  training_set_loss 2116.01910007246397071867  testing_set_loss 2900.85308260187775886152
epoch 733  training_set_loss 2115.97352010104532382684  testing_set_loss 2900.84975918545978856855
epoch 734  training_set_loss 2115.91656416283012731583  testing_set_loss 2900.83413107201567981974
epoch 735  training_set_loss 2115.85716885916372120846  testing_set_loss 2900.87236020942282266333
epoch 736  training_set_loss 2115.78497418224424109212  testing_set_loss 2900.85629990161578461993
epoch 737  training_set_loss 2115.69949183197877573548  testing_set_loss 2900.99559901902239289484
epoch 738  training_set_loss 2115.65534830305159630370  testing_set_loss 2901.01378865151764330221
epoch 739  training_set_loss 2115.59898760748001222964  testing_set_loss 2901.15871749639427434886
epoch 740  training_set_loss 2115.53667301858558857930  testing_set_loss 2901.15770741357073347899
epoch 741  training_set_loss 2115.47210087244548049057  testing_set_loss 2901.17210096374674321851
epoch 742  training_set_loss 2115.38912753556178358849  testing_set_loss 2901.12809953382020466961
epoch 743  training_set_loss 2115.34373093073827476474  testing_set_loss 2901.04500414354470194667
epoch 744  training_set_loss 2115.29168226152160059428  testing_set_loss 2901.02653832333999162074
epoch 745  training_set_loss 2115.23169317045721982140  testing_set_loss 2900.99050132156526160543
epoch 746  training_set_loss 2115.15932410900131799281  testing_set_loss 2900.93921586804481194122
epoch 747  training_set_loss 2115.06548082814060762757  testing_set_loss 2901.00554972658073893399
epoch 748  training_set_loss 2115.01108390509762102738  testing_set_loss 2900.98237067820537049556
epoch 749  training_set_loss 2114.94550076444329533842  testing_set_loss 2900.98929761644103564322
epoch 750  training_set_loss 2114.86606600600043748273  testing_set_loss 2900.99188178429994877661
epoch 751  training_set_loss 2114.77077810001082980307  testing_set_loss 2900.98771433203046399285
epoch 752  training_set_loss 2114.71441448782115912763  testing_set_loss 2900.91384531709672955913
epoch 753  training_set_loss 2114.64160084425020613708  testing_set_loss 2900.82711641508740285644
epoch 754  training_set_loss 2114.55512921159606776200  testing_set_loss 2900.83480056135022095987
epoch 755  training_set_loss 2114.45633762568741076393  testing_set_loss 2900.88405136404117001803
epoch 756  training_set_loss 2114.33606330773227455211  testing_set_loss 2900.85600316729460246279
epoch 757  training_set_loss 2114.25115457720130507369  testing_set_loss 2900.91005228756512224209
epoch 758  training_set_loss 2114.14482437965807548608  testing_set_loss 2900.85944634181942092255
epoch 759  training_set_loss 2114.01637787971276338794  testing_set_loss 2900.94107996733237087028
epoch 760  training_set_loss 2113.82979828894940510509  testing_set_loss 2900.75785074944951702491
epoch 761  training_set_loss 2113.58629235205717122881  testing_set_loss 2900.83003339062042869045
epoch 762  training_set_loss 2113.41271094404146424495  testing_set_loss 2900.80267582051965291612
epoch 763  training_set_loss 2113.17247580880257373792  testing_set_loss 2900.70453586023313619080
epoch 764  training_set_loss 2112.86000665601795844850  testing_set_loss 2900.68433592289557054755
epoch 765  training_set_loss 2112.48564377054981378024  testing_set_loss 2900.71400502080859951093
epoch 766  training_set_loss 2112.12355237498604765278  testing_set_loss 2900.77180190817307448015
epoch 767  training_set_loss 2111.95851483753585853265  testing_set_loss 2900.84924980468849753379
epoch 768  training_set_loss 2111.80407199867249801173  testing_set_loss 2900.93664544314106024103
epoch 769  training_set_loss 2111.61408112663775682449  testing_set_loss 2901.05940239544133874006
epoch 770  training_set_loss 2111.38260199844899034360  testing_set_loss 2901.09296057479696173687
epoch 771  training_set_loss 2111.09502832499765645480  testing_set_loss 2901.19844220270806545159
epoch 772  training_set_loss 2110.93831141302689502481  testing_set_loss 2901.01894849684049404459
epoch 773  training_set_loss 2110.74330047401963383891  testing_set_loss 2900.82616536864361478365
epoch 774  training_set_loss 2110.51710586010085535236  testing_set_loss 2900.63445707589335142984
epoch 775  training_set_loss 2110.25319431964135219459  testing_set_loss 2900.65714327859132026788
epoch 776  training_set_loss 2110.10808794217473405297  testing_set_loss 2900.57692801503299051546
epoch 777  training_set_loss 2109.95895789379756024573  testing_set_loss 2900.39736614093908428913
epoch 778  training_set_loss 2109.78995150690934679005  testing_set_loss 2900.38032619408295431640
epoch 779  training_set_loss 2109.59754857454663579119  testing_set_loss 2900.29081838741240062518
epoch 780  training_set_loss 2109.38354477898474215181  testing_set_loss 2900.08293252037947240751
epoch 781  training_set_loss 2109.27123237816931577981  testing_set_loss 2900.01205164864222751930
epoch 782  training_set_loss 2109.14379223761943649151  testing_set_loss 2899.68563696283035824308
epoch 783  training_set_loss 2108.99292395810334710404  testing_set_loss 2899.50435932162054086803
epoch 784  training_set_loss 2108.81784773845265590353  testing_set_loss 2899.22634658822244091425
epoch 785  training_set_loss 2108.60764923764054401545  testing_set_loss 2898.96409709168165136361
epoch 786  training_set_loss 2108.47990654503291807487  testing_set_loss 2898.81753698748480019276
epoch 787  training_set_loss 2108.31289513174397143302  testing_set_loss 2898.57901345573236540076
epoch 788  training_set_loss 2108.11702316241371590877  testing_set_loss 2898.46332498964738988434
epoch 789  training_set_loss 2107.88581645521298923995  testing_set_loss 2898.27070777817789348774
epoch 790  training_set_loss 2107.60247422564589214744  testing_set_loss 2897.88916458956236965605
epoch 791  training_set_loss 2107.43033101307491961052  testing_set_loss 2897.78671915617542254040
epoch 792  training_set_loss 2107.22617661824369861279  testing_set_loss 2897.68813014858324095258
epoch 793  training_set_loss 2106.97552610341199397226  testing_set_loss 2897.37657941525321803056
epoch 794  training_set_loss 2106.73715891456322424347  testing_set_loss 2897.13447013759287074208
epoch 795  training_set_loss 2106.48902752283038353198  testing_set_loss 2896.72897230770331589156
epoch 796  training_set_loss 2106.36293169911277800566  testing_set_loss 2896.53409568893766845576
epoch 797  training_set_loss 2106.22816307290167969768  testing_set_loss 2896.36614856247115312726
epoch 798  training_set_loss 2106.08306634813379787374  testing_set_loss 2896.05962053768143960042
epoch 799  training_set_loss 2105.92794358660512443748  testing_set_loss 2895.77630502555939528975
epoch 800  training_set_loss 2105.83935930199731956236  testing_set_loss 2895.62571583598401048221
epoch 801  training_set_loss 2105.75205516204277955694  testing_set_loss 2895.49153052092196958256
epoch 802  training_set_loss 2105.65777725774523787550  testing_set_loss 2895.34250248390299020684
epoch 803  training_set_loss 2105.54661946478972822661  testing_set_loss 2895.05270374874226035899
epoch 804  training_set_loss 2105.41621581364688609028  testing_set_loss 2894.78659760309255943866
epoch 805  training_set_loss 2105.34198040324554312974  testing_set_loss 2894.58364182583727597375
epoch 806  training_set_loss 2105.25647880428596181446  testing_set_loss 2894.36175986293164896779
epoch 807  training_set_loss 2105.15307548712007701397  testing_set_loss 2894.03964211515358329052
epoch 808  training_set_loss 2105.02108990578153679962  testing_set_loss 2893.73074733837347594090
epoch 809  training_set_loss 2104.87405588336423534201  testing_set_loss 2893.65692786426188831683
epoch 810  training_set_loss 2104.78406884085779893212  testing_set_loss 2893.44105609973848913796
epoch 811  training_set_loss 2104.67364293781747619505  testing_set_loss 2893.22694842903820244828
epoch 812  training_set_loss 2104.53872498627652021241  testing_set_loss 2892.98014827000497461995
epoch 813  training_set_loss 2104.37830240813673299272  testing_set_loss 2892.67859247866044825059
epoch 814  training_set_loss 2104.15357717940560178249  testing_set_loss 2892.09388695267170987790
epoch 815  training_set_loss 2104.01377399525699729566  testing_set_loss 2891.84619726067694500671
epoch 816  training_set_loss 2103.83600593961773483898  testing_set_loss 2891.72723318821772409137
epoch 817  training_set_loss 2103.63773349413895630278  testing_set_loss 2891.48103602982882875949
epoch 818  training_set_loss 2103.38633285310470455443  testing_set_loss 2890.79042005273777249386
epoch 819  training_set_loss 2103.10933203606828101329  testing_set_loss 2890.41669426069165638182
epoch 820  training_set_loss 2102.95683451947161302087  testing_set_loss 2890.18407356454281398328
epoch 821  training_set_loss 2102.79601429755075514549  testing_set_loss 2889.97228690772180925705
epoch 822  training_set_loss 2102.60040583699310445809  testing_set_loss 2889.65960025521189891151
epoch 823  training_set_loss 2102.42315330198834999464  testing_set_loss 2889.46257687730349061894
epoch 824  training_set_loss 2102.33444633274348234409  testing_set_loss 2889.36865932757200425840
epoch 825  training_set_loss 2102.25019016882515643374  testing_set_loss 2889.36445949049675618880
epoch 826  training_set_loss 2102.14302745406712347176  testing_set_loss 2889.28335811682109124376
epoch 827  training_set_loss 2102.03446437528509704862  testing_set_loss 2889.24347033129288320197
epoch 828  training_set_loss 2101.91744976627251162427  testing_set_loss 2889.18767544730280860676
epoch 829  training_set_loss 2101.85790800031463732012  testing_set_loss 2889.15927393293759450899
epoch 830  training_set_loss 2101.79303641707565475372  testing_set_loss 2889.01203941071162262233
epoch 831  training_set_loss 2101.72253302190983959008  testing_set_loss 2889.05253477789210592164
epoch 832  training_set_loss 2101.64632763040481222561  testing_set_loss 2889.05908576146339328261
epoch 833  training_set_loss 2101.56210880906883176067  testing_set_loss 2889.08392403966809069971
epoch 834  training_set_loss 2101.51399484014518748154  testing_set_loss 2889.09338324847931289696
epoch 835  training_set_loss 2101.45765631620224667131  testing_set_loss 2889.15639645761393694556
epoch 836  training_set_loss 2101.39540117600154189859  testing_set_loss 2889.09088823131560275215
epoch 837  training_set_loss 2101.32501798572093321127  testing_set_loss 2889.03692450215203280095
epoch 838  training_set_loss 2101.24379926907749904785  testing_set_loss 2888.97543694612249964848
epoch 839  training_set_loss 2101.19868250402168996516  testing_set_loss 2889.01775389850990904961
epoch 840  training_set_loss 2101.14418826400333273341  testing_set_loss 2889.03055831443180068163
epoch 841  training_set_loss 2101.08105332005879972712  testing_set_loss 2889.05041076131828958751
epoch 842  training_set_loss 2101.01035938476434239419  testing_set_loss 2889.04743439747289812658
epoch 843  training_set_loss 2100.92320717721577238990  testing_set_loss 2889.05346752039577040705
epoch 844  training_set_loss 2100.87476289718551925034  testing_set_loss 2889.00723096304636783316
epoch 845  training_set_loss 2100.81686672435080254218  testing_set_loss 2889.03033314946560494718
epoch 846  training_set_loss 2100.74689629784552380443  testing_set_loss 2888.96132959019269037526
epoch 847  training_set_loss 2100.66392306259740507812  testing_set_loss 2888.97046671302814502269
epoch 848  training_set_loss 2100.61951950401044086902  testing_set_loss 2888.95997478696563121048
epoch 849  training_set_loss 2100.56804587219221502892  testing_set_loss 2888.90878623120988777373
epoch 850  training_set_loss 2100.50679600126522927894  testing_set_loss 2888.87880201988173212158
epoch 851  training_set_loss 2100.43682879139760188991  testing_set_loss 2888.81520300917827626108
epoch 852  training_set_loss 2100.34906208617076117662  testing_set_loss 2888.73163511927305080462
epoch 853  training_set_loss 2100.29820691346003513900  testing_set_loss 2888.68048030978798124124
epoch 854  training_set_loss 2100.23448761021199970855  testing_set_loss 2888.55831765465700300410
epoch 855  training_set_loss 2100.16026768646270284080  testing_set_loss 2888.34155413658481847961
epoch 856  training_set_loss 2100.05488118456378288101  testing_set_loss 2888.17524866487838153262
epoch 857  training_set_loss 2099.93482437275315533043  testing_set_loss 2887.92723115147964563221
epoch 858  training_set_loss 2099.85827099287689634366  testing_set_loss 2887.78410559481517339009
epoch 859  training_set_loss 2099.77432803172223430010  testing_set_loss 2887.61075216769677354023
epoch 860  training_set_loss 2099.66357271589959054836  testing_set_loss 2887.35175727428304526256
epoch 861  training_set_loss 2099.51415415137762465747  testing_set_loss 2887.20886425602338931640
epoch 862  training_set_loss 2099.32255285926794385887  testing_set_loss 2886.97676772017302937456
epoch 863  training_set_loss 2099.19452785481871615048  testing_set_loss 2886.86157591860455795540
epoch 864  training_set_loss 2099.02322810338364433846  testing_set_loss 2886.78518020059709670022
epoch 865  training_set_loss 2098.81182540991176210809  testing_set_loss 2886.63580086960791959427
epoch 866  training_set_loss 2098.50362716378958793939  testing_set_loss 2886.41167652339572669007
epoch 867  training_set_loss 2098.15081971518611680949  testing_set_loss 2886.09485028660583338933
epoch 868  training_set_loss 2097.94435832793624285841  testing_set_loss 2885.98948934439567892696
epoch 869  training_set_loss 2097.70594653714715605020  testing_set_loss 2885.82881393521074642194
epoch 870  training_set_loss 2097.43859716124507031054  testing_set_loss 2885.64037660974145182990
epoch 871  training_set_loss 2097.15587103530788226635  testing_set_loss 2885.40414926987159560667
epoch 872  training_set_loss 2097.00298245727253743098  testing_set_loss 2885.38024487129541739705
epoch 873  training_set_loss 2096.83737562202759363572  testing_set_loss 2885.18478390266272981535
epoch 874  training_set_loss 2096.66694463597741560079  testing_set_loss 2885.09348255482700551511
epoch 875  training_set_loss 2096.48067960634125483921  testing_set_loss 2884.94380858879321749555
epoch 876  training_set_loss 2096.27159267227716554771  testing_set_loss 2884.84275290693994975300
epoch 877  training_set_loss 2096.13813955931527743815  testing_set_loss 2884.76333791892830049619
epoch 878  training_set_loss 2095.98186761067518091295  testing_set_loss 2884.74218645183964326861
epoch 879  training_set_loss 2095.77009921402986947214  testing_set_loss 2884.46362883071469696006
epoch 880  training_set_loss 2095.51374456602161444607  testing_set_loss 2884.30166535448324793833
epoch 881  training_set_loss 2095.18583381987536995439  testing_set_loss 2884.15055806955524531077
epoch 882  training_set_loss 2094.98025362611633681809  testing_set_loss 2884.07862635819356000866
epoch 883  training_set_loss 2094.72868924338081342285  testing_set_loss 2883.85411310057088485337
epoch 884  training_set_loss 2094.43013723291051064734  testing_set_loss 2883.66664722123641695362
epoch 885  training_set_loss 2094.09753638611391579616  testing_set_loss 2883.35296473199377942365
epoch 886  training_set_loss 2093.74471340535365015967  testing_set_loss 2883.07606839017716993112
epoch 887  training_set_loss 2093.56960257387027013465  testing_set_loss 2882.80462545774298632750
epoch 888  training_set_loss 2093.38850861897071808926  testing_set_loss 2882.46609841566078102915
epoch 889  training_set_loss 2093.18984653998631983995  testing_set_loss 2882.19892251278952244320
epoch 890  training_set_loss 2092.97641083091957625584  testing_set_loss 2881.85293730175681048422
epoch 891  training_set_loss 2092.74185708179675202700  testing_set_loss 2881.47651907218050837400
epoch 892  training_set_loss 2092.59716874323839874705  testing_set_loss 2881.24958553799297078513
epoch 893  training_set_loss 2092.41512736760478219367  testing_set_loss 2880.91631226339268323500
epoch 894  training_set_loss 2092.18852769485101816826  testing_set_loss 2880.52108970128210785333
epoch 895  training_set_loss 2091.88164456966023863060  testing_set_loss 2880.05709485132092595450
epoch 896  training_set_loss 2091.67564911505814961856  testing_set_loss 2879.81243130815937547595
epoch 897  training_set_loss 2091.40490532757939945441  testing_set_loss 2879.54643313270025828388
epoch 898  training_set_loss 2091.04718088349318350083  testing_set_loss 2879.18109636388453509426
epoch 899  training_set_loss 2090.60856707059019754524  testing_set_loss 2878.61379174051990048611
epoch 900  training_set_loss 2090.12003001777884492185  testing_set_loss 2878.02882130612488253973
epoch 901  training_set_loss 2089.88217637359957734589  testing_set_loss 2877.65247360930197828566
epoch 902  training_set_loss 2089.64304753870874264976  testing_set_loss 2877.31733587519784123288
epoch 903  training_set_loss 2089.42379074505379321636  testing_set_loss 2876.97361627401915029623
epoch 904  training_set_loss 2089.21386838375156003167  testing_set_loss 2876.53293129451458298718
epoch 905  training_set_loss 2088.98329128387513264897  testing_set_loss 2876.05021482619031303329
epoch 906  training_set_loss 2088.85349306702846661210  testing_set_loss 2875.75435090362225309946
epoch 907  training_set_loss 2088.69766051676015194971  testing_set_loss 2875.37676845223541022278
epoch 908  training_set_loss 2088.52113937918102237745  testing_set_loss 2875.09658911846872797469
epoch 909  training_set_loss 2088.29953155128123398754  testing_set_loss 2874.57368719047053673421
epoch 910  training_set_loss 2088.01362575934899723507  testing_set_loss 2873.72468110351292125415
epoch 911  training_set_loss 2087.84941132930725871120  testing_set_loss 2873.30159265202155438601
epoch 912  training_set_loss 2087.63870229910116904648  testing_set_loss 2872.83844084917473082896
epoch 913  training_set_loss 2087.37745655730577709619  testing_set_loss 2872.28603659728560160147
epoch 914  training_set_loss 2087.00585942617317414260  testing_set_loss 2871.26894366369924682658
epoch 915  training_set_loss 2086.56508866867125107092  testing_set_loss 2870.58078416401940557989
epoch 916  training_set_loss 2086.28601300608033852768  testing_set_loss 2870.30918878503098312649
epoch 917  training_set_loss 2085.94978536833377802395  testing_set_loss 2870.02416749586200239719
epoch 918  training_set_loss 2085.53335327912782304338  testing_set_loss 2869.93685485650166810956
epoch 919  training_set_loss 2085.06030211892675652052  testing_set_loss 2869.63075811814269400202
epoch 920  training_set_loss 2084.80888578246276665595  testing_set_loss 2869.41881739003338225302
epoch 921  training_set_loss 2084.51551828315587044926  testing_set_loss 2869.39142874094113722094
epoch 922  training_set_loss 2084.19044573311794010806  testing_set_loss 2869.23561820846907721716
epoch 923  training_set_loss 2083.83414602213861144264  testing_set_loss 2869.08496056792500894517
epoch 924  training_set_loss 2083.44726125270699412795  testing_set_loss 2868.99536666279391283751
epoch 925  training_set_loss 2083.24412825173885721597  testing_set_loss 2868.79922862426747087738
epoch 926  training_set_loss 2083.00235088503632141510  testing_set_loss 2868.74768098405502314563
epoch 927  training_set_loss 2082.74067948525680549210  testing_set_loss 2868.63312337198385648662
epoch 928  training_set_loss 2082.45227594109519486665  testing_set_loss 2868.42879465687519768835
epoch 929  training_set_loss 2082.11643972251931700157  testing_set_loss 2868.16609862512041217997
epoch 930  training_set_loss 2081.91937142340884747682  testing_set_loss 2867.95548817493181559257
epoch 931  training_set_loss 2081.67937692426494322717  testing_set_loss 2867.71452040276244588313
epoch 932  training_set_loss 2081.39547107607631915016  testing_set_loss 2867.42015776939388160827
epoch 933  training_set_loss 2081.07125836518525829888  testing_set_loss 2866.99285808655895380070
epoch 934  training_set_loss 2080.70035182155970687745  testing_set_loss 2866.50654668435072380817
epoch 935  training_set_loss 2080.48835005459795866045  testing_set_loss 2866.34726083445593758370
epoch 936  training_set_loss 2080.24381476627422671299  testing_set_loss 2866.18741129993031790946
epoch 937  training_set_loss 2079.95851281084787842701  testing_set_loss 2866.17641622105475107674
epoch 938  training_set_loss 2079.67806145274744267226  testing_set_loss 2865.86870339652114125784
epoch 939  training_set_loss 2079.38749357808501372347  testing_set_loss 2865.27573817490201690816
epoch 940  training_set_loss 2079.22738666925488359993  testing_set_loss 2865.07986186975267628441
epoch 941  training_set_loss 2079.05568324187561302097  testing_set_loss 2864.90839536066505388590
epoch 942  training_set_loss 2078.85951600692578722374  testing_set_loss 2864.83214647107706696261
epoch 943  training_set_loss 2078.64788417855970692472  testing_set_loss 2864.64067406569665763527
epoch 944  training_set_loss 2078.51428859095858570072  testing_set_loss 2864.55025956247845897451
epoch 945  training_set_loss 2078.36658452121400841861  testing_set_loss 2864.39895986671717764693
epoch 946  training_set_loss 2078.20078765538391962764  testing_set_loss 2864.23992906487001164351
epoch 947  training_set_loss 2078.00579090758219535928  testing_set_loss 2864.07404868418961996213
epoch 948  training_set_loss 2077.75233446429547257139  testing_set_loss 2863.83088579156083142152
epoch 949  training_set_loss 2077.61666354842918735812  testing_set_loss 2863.76020346661744042649
epoch 950  training_set_loss 2077.44822014159763057251  testing_set_loss 2863.64508796357040409930
epoch 951  training_set_loss 2077.24386330807737977011  testing_set_loss 2863.57277597685424552765
epoch 952  training_set_loss 2076.98689725514077508706  testing_set_loss 2863.47855129546405805741
epoch 953  training_set_loss 2076.62541133369131785003  testing_set_loss 2863.19118027050444652559
epoch 954  training_set_loss 2076.42621073560121658375  testing_set_loss 2863.08314164587090999703
epoch 955  training_set_loss 2076.19795265467791978153  testing_set_loss 2863.06978373931769965566
epoch 956  training_set_loss 2075.94072194912132545142  testing_set_loss 2862.91520445259402549709
epoch 957  training_set_loss 2075.62867703794290719088  testing_set_loss 2862.66939846814784687012
epoch 958  training_set_loss 2075.29947130399568777648  testing_set_loss 2862.43144006416650881874
epoch 959  training_set_loss 2075.14021275823915857472  testing_set_loss 2862.42534694471987677389
epoch 960  training_set_loss 2074.96224928044057378429  testing_set_loss 2862.52995794956677855225
epoch 961  training_set_loss 2074.77689790211661602370  testing_set_loss 2862.48242308129147204454
epoch 962  training_set_loss 2074.58685825005113656516  testing_set_loss 2862.48582989194528636290
epoch 963  training_set_loss 2074.37883023740459975670  testing_set_loss 2862.49951029583507988718
epoch 964  training_set_loss 2074.27391564068466323079  testing_set_loss 2862.42108274020756653044
epoch 965  training_set_loss 2074.14208760457040625624  testing_set_loss 2862.22448805300791718764
epoch 966  training_set_loss 2074.00872296918214487960  testing_set_loss 2862.10382561226515463204
epoch 967  training_set_loss 2073.88615000125082588056  testing_set_loss 2861.97820420651169115445
epoch 968  training_set_loss 2073.79122176211876649177  testing_set_loss 2861.96399522446108676377
epoch 969  training_set_loss 2073.69625004577983418130  testing_set_loss 2861.81236627151156426407
epoch 970  training_set_loss 2073.59148061922314809635  testing_set_loss 2861.66165512075167498551
epoch 971  training_set_loss 2073.45718485136785602663  testing_set_loss 2861.36778096973603169317
epoch 972  training_set_loss 2073.29771276371775456937  testing_set_loss 2860.89728781625899500796
epoch 973  training_set_loss 2073.21383850830216033501  testing_set_loss 2860.75091933575868097250
epoch 974  training_set_loss 2073.11266634796083963010  testing_set_loss 2860.56015277996448276099
epoch 975  training_set_loss 2072.97971883261698167189  testing_set_loss 2860.22546800114787401981
epoch 976  training_set_loss 2072.81469744542619082495  testing_set_loss 2859.81024959773776572547
epoch 977  training_set_loss 2072.62005533982710403507  testing_set_loss 2859.43093376554497808684
epoch 978  training_set_loss 2072.50299669341711705783  testing_set_loss 2859.22231032446688914206
epoch 979  training_set_loss 2072.37677330126780361752  testing_set_loss 2859.12632931058442409267
epoch 980  training_set_loss 2072.21114135106836329214  testing_set_loss 2858.84056990956287336303
epoch 981  training_set_loss 2072.01369209726362896618  testing_set_loss 2858.59249737388608991751
epoch 982  training_set_loss 2071.77313042078731086804  testing_set_loss 2858.14674601924753005733
epoch 983  training_set_loss 2071.63061198494142445270  testing_set_loss 2857.93876254884389709332
epoch 984  training_set_loss 2071.46304269216943794163  testing_set_loss 2857.68390425763300299877
epoch 985  training_set_loss 2071.25803917147186439252  testing_set_loss 2857.41035204087575039011
epoch 986  training_set_loss 2071.02210520769904178451  testing_set_loss 2857.13782813516081660055
epoch 987  training_set_loss 2070.74711023593272329890  testing_set_loss 2856.81370420887014915934
epoch 988  training_set_loss 2070.59181061971366943908  testing_set_loss 2856.62438699025005917065
epoch 989  training_set_loss 2070.40687044853802944999  testing_set_loss 2856.40863541513044765452
epoch 990  training_set_loss 2070.19729965489705136861  testing_set_loss 2856.23092097075641504489
epoch 991  training_set_loss 2069.92442289783366504707  testing_set_loss 2856.03590604834244004451
epoch 992  training_set_loss 2069.76433199812117891270  testing_set_loss 2855.83720523912415956147
epoch 993  training_set_loss 2069.55320888717642446863  testing_set_loss 2855.59342864289192220895
epoch 994  training_set_loss 2069.33371304747379326727  testing_set_loss 2855.49458270752757016453
epoch 995  training_set_loss 2069.04379252215949236415  testing_set_loss 2855.27687930541469540913
epoch 996  training_set_loss 2068.71586115992522536544  testing_set_loss 2854.93648379168098472292
epoch 997  training_set_loss 2068.52572591461012052605  testing_set_loss 2854.80723340568556523067
epoch 998  training_set_loss 2068.31255625531503028469  testing_set_loss 2854.64090747260297575849
epoch 999  training_set_loss 2068.06864883550179001759  testing_set_loss 2854.52749768253215734148
epoch 1000  training_set_loss 2067.81959144025040586712  testing_set_loss 2854.39191768790306014125
epoch 1001  training_set_loss 2067.57815365371834559483  testing_set_loss 2854.27621748182446026476
epoch 1002  training_set_loss 2067.46625849867768920376  testing_set_loss 2854.17758311028728712699
epoch 1003  training_set_loss 2067.34836777521286421688  testing_set_loss 2854.13211333760682464344
epoch 1004  training_set_loss 2067.21937787802062302944  testing_set_loss 2854.08204100981674855575
epoch 1005  training_set_loss 2067.07944072611326191691  testing_set_loss 2854.02059872886911762180
epoch 1006  training_set_loss 2066.93782103083685797174  testing_set_loss 2853.95892602914773306111
epoch 1007  training_set_loss 2066.86807856566019836464  testing_set_loss 2853.96757702651302679442
epoch 1008  training_set_loss 2066.78842035080970163108  testing_set_loss 2853.99025679512942588190
epoch 1009  training_set_loss 2066.70502633053592944634  testing_set_loss 2854.05277618470017841901
epoch 1010  training_set_loss 2066.61610958014261996141  testing_set_loss 2854.08448859420923326979
epoch 1011  training_set_loss 2066.51404335401457501575  testing_set_loss 2854.06720592777855927125
epoch 1012  training_set_loss 2066.46197000436950474977  testing_set_loss 2854.07808654305335949175
epoch 1013  training_set_loss 2066.40625543036048838985  testing_set_loss 2854.07322478124888220918
epoch 1014  training_set_loss 2066.34180478462076280266  testing_set_loss 2853.98441708828977425583
epoch 1015  training_set_loss 2066.26981813787779174163  testing_set_loss 2853.96075897400987742003
epoch 1016  training_set_loss 2066.22704393761569008348  testing_set_loss 2854.05521558983309660107
epoch 1017  training_set_loss 2066.17789855063483628328  testing_set_loss 2854.10272486134499558830
epoch 1018  training_set_loss 2066.12576584239968724432  testing_set_loss 2854.07340095771041887929
epoch 1019  training_set_loss 2066.06593207576679560589  testing_set_loss 2854.09334515813770849491
epoch 1020  training_set_loss 2065.99655297604613224394  testing_set_loss 2854.06558255916024791077
epoch 1021  training_set_loss 2065.95770692580435934360  testing_set_loss 2854.00990107003235607408
epoch 1022  training_set_loss 2065.91603617912733170670  testing_set_loss 2853.93020372216915347963
epoch 1023  training_set_loss 2065.86552189098256349098  testing_set_loss 2853.88011639250089501729
epoch 1024  training_set_loss 2065.81114735006985938526  testing_set_loss 2853.84281698712948127650
epoch 1025  training_set_loss 2065.75058444322121431469  testing_set_loss 2853.79621422663876728620
epoch 1026  training_set_loss 2065.71948091761305477121  testing_set_loss 2853.69999139856372494251
epoch 1027  training_set_loss 2065.68340480371443845797  testing_set_loss 2853.69562067749484413071
epoch 1028  training_set_loss 2065.64191013143090458470  testing_set_loss 2853.68274451045772366342
epoch 1029  training_set_loss 2065.58930753139475200442  testing_set_loss 2853.72747909018198697595
epoch 1030  training_set_loss 2065.53291903889248715132  testing_set_loss 2853.76426406493783360929
epoch 1031  training_set_loss 2065.50409444585466189892  testing_set_loss 2853.71905143060666887322
epoch 1032  training_set_loss 2065.46734755069519451354  testing_set_loss 2853.73439994875297998078
epoch 1033  training_set_loss 2065.42842763872113209800  testing_set_loss 2853.65286552746238157852
epoch 1034  training_set_loss 2065.38344407058457363746  testing_set_loss 2853.57745632743717578705
epoch 1035  training_set_loss 2065.33361376136781473178  testing_set_loss 2853.56731075811649134266
epoch 1036  training_set_loss 2065.30263382876137256972  testing_set_loss 2853.53996758508219500072
epoch 1037  training_set_loss 2065.26836567142299827537  testing_set_loss 2853.55343661417145995074
epoch 1038  training_set_loss 2065.22926503877170034684  testing_set_loss 2853.58375751111452700570
epoch 1039  training_set_loss 2065.18936624477282748558  testing_set_loss 2853.50168588870019448223
epoch 1040  training_set_loss 2065.16302639925061157555  testing_set_loss 2853.42642667599557171343
epoch 1041  training_set_loss 2065.13343219330727151828  testing_set_loss 2853.40699033125292771729
epoch 1042  training_set_loss 2065.10008031239522097167  testing_set_loss 2853.37165886999173380900
epoch 1043  training_set_loss 2065.05877816134943714133  testing_set_loss 2853.29802616072811360937
epoch 1044  training_set_loss 2065.01601784410377149470  testing_set_loss 2853.28192933674199593952
epoch 1045  training_set_loss 2064.99046968659649792244  testing_set_loss 2853.26450495990229683230
epoch 1046  training_set_loss 2064.95954886264416927588  testing_set_loss 2853.26659857025742894621
epoch 1047  training_set_loss 2064.92125618434556599823  testing_set_loss 2853.34939876959151661140
epoch 1048  training_set_loss 2064.88068649129900222761  testing_set_loss 2853.46777047471914556809
epoch 1049  training_set_loss 2064.83337430428491643397  testing_set_loss 2853.41480602824822199182
epoch 1050  training_set_loss 2064.80568407650389417540  testing_set_loss 2853.35314872827166254865
